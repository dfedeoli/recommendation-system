---
title: "Recommendation Systems - Rating Predictions with MovieLens"
author: "Danilo Ferreira de Oliveira"
date: "04/12/2021"
output: 
  pdf_document:
    highlight: tango
    latex_engine: xelatex
    number_sections: true
    toc: true
    toc_depth: 3
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = '>>', tidy = TRUE, fig.pos = "!H", out.extra = "") 
```

# Introduction

For this project, we will be creating a movie recommendation system using the MovieLens dataset. You can find the [entire latest MovieLens dataset here](https://grouplens.org/datasets/movielens/latest/). You will be creating your own recommendation system using all the tools shown throughout the courses in the HarvardX: Data Science Professional Certificate series. We will use the [10M version of the MovieLens dataset](https://grouplens.org/datasets/movielens/10m/) to make the computation a little easier.

You will train a machine learning algorithm using the inputs in one subset to predict movie ratings in the validation set. 

Recommendation systems are more complicated machine learning challenges because each outcome has a different set of predictors. For example, different users rate a different number of movies and rate different movies. By evaluating the movies specific users like to watch, we can predict ratings they would give for movies they haven't seen, and then recommend them those with relatively high ratings.

We will firstly follow the approach made by Professor Rafael Irrizarry in [Introduction to Data Science](https://rafalab.github.io/dsbook), and continue it with the addition of genre effects, and a model that uses matrix factorization.

To compare different models or to see how well we are doing compared to a baseline, we will use root mean squared error (RMSE) as our loss function. We can interpret RMSE similar to standard deviation.

ou will use the following code to generate your datasets. Develop your algorithm using the edx set. For a final test of your final algorithm, predict movie ratings in the validation set (the final hold-out test set) as if they were unknown. RMSE will be used to evaluate how close your predictions are to the true values in the validation set (the final hold-out test set).

# Analysis

## Getting the data

```{r install_packages, echo=FALSE, warning=FALSE,message=FALSE}
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(rlist)) install.packages("rlist", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(recosystem)) install.packages("recosystem", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(tibble)) install.packages("tibble", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(cowplot)) install.packages("cowplot", repos = "http://cran.us.r-project.org")
```

 Before we can start gathering data, we will first load the R libraries we will need along all the report.

```{r libraries, warning=FALSE,message=FALSE}
library(caret)     
library(tidyverse) 
library(ggplot2)
library(lubridate)
library(stringr)
library(recosystem)
library(data.table)
library(kableExtra)
library(tibble)
```

Then, we download the MovieLens data and run code provided by HarvardX to generate the datasets for training and evaluating the final model. The `edx` set will be used all along the report to train and choose the best model for rating predictions, while the `validation` set will only be used at the end of to evaluate the quality of the final model with the root mean square error. 

```{r getting_data, eval=FALSE}
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")
```

The validation set will be 10% of the MovieLens data obtained. We need to make sure the user and movie IDs in `validation` are also in `edx`, so we apply the `semi_join()` transformations. Then, we add rows removed from the validation set back into the training set. Lastly, we clean variables to recover RAM space.

```{r creating_edx_validation, warning=FALSE, message=FALSE, results='hide', eval=FALSE}
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

We can now start analyzing the dataframe `edx`.

```{r, echo=FALSE}
load('rda/edx.rda')
load('rda/validation.rda')
```


## Exploratory data analysis and visualization

### Exploring the variables

Let's take a look at the columns in our dataset. 

```{r head_of_edx}
edx %>% as_tibble() 
```

It is possible to see that columns `title` and `genres` are of character class ($<$chr$>$), while `movieId` and `rating` are numeric ($<$dbl$>$), `userId` and `timestamp` are integers ($<$int$>$).

We should analyze the quantity of unique values for each column variable. 

```{r summary_n_distinct}
edx %>% summarize(n_userIds = n_distinct(userId), n_movieIds = n_distinct(movieId),
                  n_titles = n_distinct(title), n_genres = n_distinct(genres)) %>% 
  kbl(booktabs = TRUE,
      caption = "Distinct values for userId, movieId, titles and genres") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

Note that there are more `movieId` unique variables than there are `title` ones. We need to investigate why.

```{r movies_with_multiple_Ids}
edx %>% group_by(title) %>% 
  summarize(n_movieIds = n_distinct(movieId)) %>% filter(n_movieIds > 1) %>% 
  kbl(booktabs = TRUE, caption = "Movies with more than one ID") %>% 
  kable_styling(latex_options = c("striped","hold_position"))
```

So we discover that _War of the Worlds (2005)_ has two distinct `movieId` numbers. Let's see how many reviews for each of them there are.

```{r movieIds_for_WW2005, echo=FALSE}
edx %>% filter(title=='War of the Worlds (2005)') %>% 
  group_by(movieId) %>% summarize(title=title[1], genres=genres[1], n=n()) %>%  # Count of reviews for each movieId
  kbl(booktabs = TRUE, caption = "Different values of movieId and genres for \\textit{War of the Worlds (2005)}") %>% kable_styling(latex_options = c("striped","hold_position"))
```

The second `movieId` received only 28 reviews. It is a low value and the validation set possibly has both ID values for the movie, so we won't meddle with it.

### Ratings

We can now visualize the data. Let's see how the ratings distribution is configured.

```{r ratings_distribution, echo=FALSE, fig.height=3, fig.fullwidth=TRUE, fig.align='center', fig.cap = "Distribution of ratings"}
edx %>% mutate(stars=ifelse(rating %in% c(1:5),'full','half')) %>% 
  ggplot(aes(factor(rating),fill=stars)) + geom_bar() + scale_fill_hue(c=80) + 
  labs(x='rating')
```

From the plot above, we can affirm that full star ratings are more common than those with half stars, since 4, 3 and 5 are the most frequent ones.

### Movie and user IDs

Let's evaluate the frequency distribution of reviews that each user gives and reviews each movie receives.

```{r hist_count, echo=FALSE, fig.align='center', fig.height=3, fig.cap= 'Frequency of reviews given by a specific user (left) and of reviews received by a specific movie (right)'}
p1 <- edx %>% count(userId) %>% ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + scale_x_log10() + 
  labs(x='reviews given by user') # Number of reviews from each user

p2 <- edx %>% count(movieId) %>% ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + scale_x_log10() +
  labs(x='reviews received by movie',y='') # Number of reviews of each movie

gridExtra::grid.arrange(p1, p2, ncol = 2)
rm(p1,p2)
```

The x-axis from the plots above are both on a logarithmic scale, and the one on the left is **rightly skewed**. If we analyze them, we can say that the majority of the users give from 20 to 200 reviews, while each movie frequently receive between 20 to a 1000 reviews, which is a pretty broad range.

Plotting histograms for average ratings, we get the figures below. From the top plot, it is possible to see that it is rare for a user to average rating movies below 2, since there are few users to the left of that number (only `r edx %>% group_by(userId) %>% summarize(avg=mean(rating)) %>% filter(avg< 2) %>% dim() %>% .[1]` of all `r edx %>% summarize(n_userIds = n_distinct(userId)) %>% .[1,1]` in the dataset, to be more exact). 

From all 10,676 movies, we can see that only `r edx %>% group_by(movieId) %>% summarize(avg=mean(rating)) %>% filter(avg< 1.5) %>% dim() %>% .[1]` of them average below 1.5, and `r edx %>% group_by(movieId) %>% summarize(avg=mean(rating)) %>% filter(avg> 4.25) %>% dim() %>% .[1]` films average over 4.25 rating.

```{r hist_avg_rating, echo=FALSE, fig.align='center', fig.height=3, fig.cap='Frequency of average rating a user gives (top) and average rating a movie receives (bottom)'}
p1 <- edx %>% group_by(userId) %>% summarize(avg=mean(rating)) %>% 
  ggplot(aes(avg)) + geom_histogram(bins=35, color='black') +
  labs(x='average rating given by user') #average rating a user gives

p2 <- edx %>% group_by(movieId) %>% summarize(avg=mean(rating)) %>% 
  ggplot(aes(avg)) + geom_histogram(bins=35, color='black') +
  labs(x='average rating received by movie') # Average rating received by a movie

cowplot::plot_grid(p1,p2,ncol=1, align='v')
rm(p1,p2)
```

Let's now explore the top 10 most reviewed movies in the dataset and how many times they were reviewed.

```{r unique_movies, echo=FALSE, fig.height=4, fig.fullwidth=TRUE, fig.align='center', fig.cap='Number of reviews for the top 10 most reviewed movies'}
unique_movies <- edx %>% group_by(title) %>% 
  summarize(title=title[1], genres=genres[1], count=n()) %>% arrange(desc(count))
unique_movies[1:10,] %>% mutate(title = str_remove(title, ", The"),
                                title = str_remove(title, " - A New Hope \\(a.k.a. Star Wars\\)")) %>% 
  ggplot(aes(x=reorder(title, count), y=count)) + geom_bar(stat = "identity") + labs(x='') + coord_flip()
```

As we can see, the most reviewed movies are normally blockbusters, and they also have really high ratings, the lowest of them being 3.66. We can infer that, the more times a movie is reviewed, the better its rating is. 

```{r movies_ratings, echo=FALSE, fig.height=4, fig.fullwidth=TRUE, fig.align='center', fig.cap='Average rating for the top 10 most reviewed movies'}
edx %>% filter(title %in% head(unique_movies$title,n=10)) %>% group_by(title) %>% 
  summarize(avg=mean(rating), count=n()) %>% mutate(title = str_remove(title, ", The"),
                                                                title = str_remove(title, " - A New Hope \\(a.k.a. Star Wars\\)")) %>% 
  ggplot(aes(x=reorder(title, count), y=avg)) + geom_bar(stat = "identity") + labs(x='',y='average rating') + coord_flip()
```


the title of the movies come along with the release date, inside parenthesis.

### Release date

Since there isn't a column for each movie's release date, we will check it by using the `str_extract` function from the `stringr` package.

```{r release_date}
release_date <- edx$title %>% str_extract('\\([0-9]{4}\\)') %>%
  str_extract('[0-9]{4}') %>% as.integer()
summary(release_date)
```

We first apply this function extracting the date along with the parenthesis, and then the number from inside them. We do it like this because by extracting only the number, movies like _2001: A Space Odyssey (1968)_ and _2001 Maniacs (2005)_ show up when searching for year 2001, for example. To go even further, it also extracts the numbers 1000 and 9000 for _House of 1000 Corpses (2003)_ and _Detroit 9000 (1973)_, respectively.

Below we can see that the earliest release date in our dataset (1915) only has one movie:

```{r released_edx, echo=FALSE}
edx <- data.frame(edx,released=release_date)
edx[which(edx$released==1915),] %>% summarize(movieId=movieId[1], title=title[1], genres=genres[1], n=n()) %>% kbl(booktabs = TRUE, linesep = "", caption = "Movie with the earliest release date in the dataset") %>% kable_styling(latex_options = c("striped","HOLD_position"))
```

We now compute the number of ratings for each movie and then plot it against the year the movie came out, using a boxplot for each year. The logarithmic transformation is applied on the y-axis (number of ratings) when creating the plot.

```{r released_count, echo=FALSE, fig.fullwidth=TRUE, fig.align='center', fig.height=4, fig.cap="Number of reviews the movies received versus the year they were released"}
edx %>% group_by(movieId) %>%
  summarize(n = n(), released = as.character(first(released))) %>%
  ggplot(aes(released, n)) + geom_boxplot(fill= 'orange', color='#808080', alpha=0.2) +
  scale_y_log10(breaks=c(1,10,100,200,500,1000,10000)) + scale_x_discrete(breaks=seq(1915,2005,5)) + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) + labs(x='year released',y='count')
```

We see that, on average, movies that came out in the mid 90's get more ratings, reaching values over 30,000 reviews. We also see that with newer movies, starting near 1998, the number of ratings decreases with year: the more recent a movie is, the less time users have had to rate it.

Now we compute the average of ratings for each movie and plot it against the release year with boxplots, similarly to before.

```{r released_avg_rating, echo=FALSE, fig.fullwidth=TRUE, fig.align='center', fig.height=4, fig.cap="Average rating the movies received versus the year they were released"}
edx %>% group_by(movieId) %>%
  summarize(n = n(), released = as.character(first(released)), avg_rating=mean(rating)) %>%
  ggplot(aes(released, avg_rating)) + geom_boxplot(fill= 'orange', color='#808080', alpha=0.2) + scale_x_discrete(breaks=seq(1915,2005,5)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) + labs(x='year released', y='average rating')
edx <- edx[,-ncol(edx),drop=FALSE]
rm(release_date)
```

It is possible to see that most movies released until 1973 normally range between 3.0 and 4.0 ratings. From then on, the range gets bigger and values get lower. The median value goes from averaging over 3.5 to near 3.25. There are even some movies from around the 2000's that average 0.5 ratings, but they are outliers and probably only received very few ratings.

### Genres and genre combinations

The `genres` variable actually represents combinations of a series of unique genres. We will count the number of reviews for all different combinations that appear in our dataset and arrange them in descending order. The table below shows, as an example, the top 7 most reviewed genre combinations.

```{r genre_combinations, echo=FALSE}
edx %>% group_by(genres) %>% summarize(count = n()) %>% arrange(desc(count)) %>% head(n=7) %>% 
  data.frame(rank=c(1:7), .) %>% kbl(booktabs = TRUE, linesep = "", caption = 'Top genre combinations rated') %>% kable_styling(latex_options = c("striped","HOLD_position"))
```

Let's check the genre effect on ratings. We will filter for genre combinations that were reviewed over a thousand times, put them in ascending order of average rating and show 15 of them, equally distant inside the arranged dataframe. The indexes to select the genres in the arranged set are represented by the variable `gcomb_15`.

```{r genre_effect, fig.fullwidth=TRUE, fig.align='center', fig.height=4, fig.cap="Distribution of average ratings for a few genre combinations possible"}
gcomb_15 <- c(1,seq(31,415,32),444) 
edx %>% group_by(genres) %>%
  summarize(n = n(), avg = mean(rating), se = sd(rating)/sqrt(n())) %>%     
  filter(n > 1000) %>% arrange(desc(avg)) %>%  .[gcomb_15,] %>%
  mutate(genres = reorder(genres, avg)) %>%
  ggplot(aes(x = genres, y = avg,
             ymin = avg - 2*se, ymax = avg + 2*se)) + geom_point() + 
  geom_errorbar() + theme(axis.text.x=element_text(angle = 30, hjust = 1)) + 
  labs(x='', y= 'average rating')
```

From the plot above, it is clear the effect genre combinations have on the rating, since they range from approximately 2.0 to over 4.5. Let's now see the effect of each genre separately.

There are 20 different genres in this dataset, and they are the following:

```{r all_genres, echo=FALSE}
rm(gcomb_15)
genre_combinations <- unique(edx$genres)     # Unique genre combinations
all_genres <- character()                    # Empty character vector
for (i in genre_combinations) {
  movie_genres <- str_split(i,'\\|')[[1]]    # Splitting the genre combinations
  for (j in movie_genres) {
    all_genres <- rlist::list.append(all_genres, j) # Appending each genre
  }
}
all_genres <- unique(all_genres)             # Unique genres in the dataset
rm(genre_combinations,movie_genres,i,j)
matrix(all_genres, nrow = 5, ncol= 4) %>% kbl(booktabs = TRUE, linesep = "", caption = 'All unique genres present in the dataset') %>% column_spec(1:4, width='8em') %>% kable_styling(latex_options =c("striped","HOLD_position"))
```

We can count how many reviews each genre have had, by applying the following function:

```{r all_genres_count}
all_genres_count <- sapply(all_genres, function(g) {
  sum(str_detect(edx$genres, g))      
}) %>% data.frame(review_count=.) %>% 
  arrange(desc(.))

```

We can see the results on the table below. The genres range from near 4 million to only 7 reviews.    

```{r all_genres_count_div, echo=FALSE}
all_genres_count_div <-data.frame(row.names(all_genres_count)[1:10], all_genres_count[1:10,],
                                  row.names(all_genres_count)[11:20], all_genres_count[11:20,]) # Dividing table in two for better placement in report
colnames(all_genres_count_div) <- c('Genre','Count','Genre','Count')
all_genres_count_div %>% kbl(booktabs = TRUE, linesep = "", caption = 'Number of reviews per genre') %>% column_spec(1:4, width='9em') %>% kable_styling(latex_options =c("striped","HOLD_position")) %>% add_header_above(c("all_genres_count[1:10]" = 2, "all_genres_count[11:20]" = 2)) # all_genres_count[1:10]
 
```

We need to investigate the `(no genres listed)` genre, which actually represents no genre at all. We can see the movies that belong to it and their respective averages and standard errors:

```{r no_genres_listed}
edx[which(edx$genres=='(no genres listed)'),] %>% 
  summarize(movieId=movieId[1], userId=userId[1], title=title[1], genres=genres[1],
            n = n(), avg = mean(rating), se = sd(rating)/sqrt(n()))
```

There is only one movie in the dataset with no genres listed, and it has only 7 reviews, giving its average rating a high standard error (0.4185). For this reason, we won't be plotting this genre's average rating along with the others.

The plot below shows the average ratings for each unique genre, as they range from under 3.3 to over 4.0.

```{r all_genres_avg_rating, echo=FALSE, fig.align='center', fig.width=6, fig.length=3.5, fig.cap='Average rating for each genre'}
all_genres_avg <- matrix(nrow=20,ncol=2)
i <- 1
for (var in all_genres) {
  vals <- edx[which(str_detect(edx$genres,as.character(var))),] %>% 
    summarize(n = n(), avg = mean(rating), se = sd(rating)/sqrt(n()))
  all_genres_avg[i,1] <- vals$avg
  all_genres_avg[i,2] <- vals$se
  i <- i+1
}
data.frame(genres=all_genres, avg=all_genres_avg[,1], se=all_genres_avg[,2]) %>% .[1:19,] %>%
  mutate(genres = reorder(genres, avg)) %>%
  ggplot(aes(x = genres, y = avg, ymin = avg - 2*se, ymax = avg + 2*se)) +  # Plotting the values and errors
  geom_point() + geom_errorbar() + theme(axis.text.x=element_text(angle = 45, hjust = 1)) + 
  labs(x='', y= 'average rating')
rm(var,i,vals,all_genres_avg)
```


We now observe the top genres reviewed in this dataset, and how many reviews each one of them have. We accounted for the genres that were reviewed over a million times, resulting in 8 distinct possibilities. The reviews count for all genre has already been shown, but we will plot the count for the top 8 genres to better visualize it, and also compare their shape with the movie count plot for the top 8. 

```{r top_genres_reviews, echo=FALSE, fig.width=6, fig.height=3, fig.fullwidth=TRUE, fig.align='center', fig.cap='Number of reviews for each of the top 8 most reviewed genres'}
top_genres <- all_genres_count %>% filter(.>=1e6) # Genres with over 1 million reviews

top_genres %>% ggplot(aes(x= row.names(.), y = review_count, fill=row.names(.))) +
  geom_bar(stat='identity') + scale_fill_hue(c=40) + theme(legend.position="none") + 
  labs(x='', y='reviews count')
```

We now plot how many different movies where reviewed for each of the top 8 genres.

```{r movies_top_genre, echo=FALSE, fig.height=3, fig.fullwidth=TRUE, fig.align='center', fig.cap='Number of movies that belongs to the top 8 most reviewed genres'}
movies_per_top_genre <- sapply(row.names(top_genres), function(g) {
  sum(str_detect(unique_movies$genres, g))
})
data.frame(movies_per_top_genre) %>% 
  ggplot(aes(x= row.names(.), y = movies_per_top_genre, fill=row.names(.))) +
  geom_bar(stat='identity') + scale_fill_hue(c=40) + theme(legend.position="none") +
  labs(x='', y='movies count')
rm(movies_per_top_genre,top_genres,unique_movies,all_genres_count)
```

The general shape and proportion of the bars pretty much look the same for both reviews and movie counts.   

### Rating date

Since `timestamp` doesn't really have any value for us as it is, we create from it another column that gives us the date and time a movie was rated.

```{r timestamp_to_date}
edx <- mutate(edx, date = as_datetime(timestamp))
edx %>% as_tibble()
```

The new column `date` is of a Date-Time class (`<dttm>`) called POSIXct, and it clearly makes it better to understand the time of rating.

We can now plot the time effect on the dataset, showing the influence the week a movie was rated on the average value. Along with ratings, we plot a LOESS smooth line to better show the relationship between variables.

```{r time_effect, message=FALSE, fig.height=4, fig.fullwidth=TRUE, fig.align='center', fig.cap='The review date effect on the average rating'}
edx %>% mutate(date = round_date(date, unit = "week")) %>%
  group_by(date) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(date, rating)) +
  geom_point() + labs(y='average rating') +
  geom_smooth()
```

The review date definitely has some effect on ratings, but it isn't significant.

## Feature engineering

Down the line, we will need all genres available in the dataset as binary values, so it is necessary to create columns for all 20 of them. For each row, we apply value 1 to their corresponding genres in the character column, and value 0 to the rest. here we see the first 6 rows, with only the movieId, title and genres of the movies, so we can compare the genres further in the analysis.

```{r binary_genres_pre_creation, tidy=TRUE}
edx[,c(2,5,6)] %>% as_tibble(.rows = 6)
```

We apply the transformation by doing the following `for()` command. We will also permanently remove the timestamp, title, genres and date columns, since we won't be needing them to train the prediction models. 

```{r binary_genres_columns, tidy=TRUE}
for (var in all_genres) {
  edx <- edx %>% mutate(genre=ifelse(str_detect(genres,as.character(var)),1,0))
  colnames(edx)[ncol(edx)] <- as.character(var)
}
rm(var)
edx <- edx[,-(4:7),drop=FALSE]
edx[,4:23] %>% as_tibble(.rows=6)
```

Comparing each row in this tibble above with the ones right before, it is possible to see that the genres in the first one are represented as 1s in the other, while genres that don't appear before are kept as zero values. The transformation occurred accurately.

## Creating training and test sets

Before training any model, we need to create a train and a test set. We first set the seed to guarantee the reproducibility of the model, and we divide the `edx` dataframe into 80-20% partitions.

```{r partitioning, warning=FALSE}
set.seed(123, sample.kind = "Rounding")
index <- createDataPartition(edx$rating, times=1, p=0.2, list= FALSE)
test <- edx[index,]
train <- edx[-index,]
```

```{r cleaning_index_edx, echo=FALSE}
rm(index)
edx <- edx %>% select(userId, movieId, rating)
```

To make sure all user and movie IDs in `test` are also in `train`, we apply the `semi_join()` function, just as we did in the beginning of the report for the first two datasets.

```{r test_semi_join}
test <- test %>% 
  semi_join(train, by = "movieId") %>%
  semi_join(train, by = "userId")
```

## Prediction models

Now that we have our training and test sets, we can start evaluating prediction models. We will begin by following the approach in Prof. Irrizarry's book, starting with just a Overall Average Rating model, going through the Movie effect, Movie + User effect and Regularized Movie + User effects models. We will also apply a method that includes Genre effect and one that performs Matrix Factorization. To evaluate their quality, we will use the RMSE function.

 If N is the number of user-movie combinations, $y_{u,i}$ is the rating for movie i by user u, and $\hat{y}_{u,i}$ is our prediction, then RMSE is defined as follows: 

$$
\mbox{RMSE} = \sqrt{\frac{1}{N} \sum_{u,i}^{N} \left( \hat{y}_{u,i} - y_{u,i} \right)^2 }
$$
which is represented by the function:

```{r rmse_function}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

### Overall Average Rating

We start with a model that assumes the same rating for all movies and all users, with all the differences explained by random variation: If $\mu$ represents the true rating for all movies and users and $\varepsilon$ represents independent errors sampled from the same distribution centered at zero, then:

$$
Y_{u,i} = \mu + \varepsilon_{u,i}
$$

In this case, the least squares estimate of $\mu$ — the estimate that minimizes the root mean squared error — is the average rating of all movies across all users.

```{r naive_rmse}
mu <- mean(train$rating)
```

In this first model, we will predict all ratings as the average rating of the training set and compare it to the actual ratings in the test set (`RMSE(test$rating, mu)`).

```{r naive_results, echo=FALSE}
rmse_results <- tibble(Method = "Just the Average", RMSE = RMSE(test$rating, mu))
kbl(rmse_results, booktabs = TRUE, caption = 'RMSE result for the Average Method') %>%  kable_styling(latex_options = c("striped","HOLD_position"))
```

We can observe that this value is actually the standard deviation of this distribution. Its value is too high, thus not acceptable.

$$
\mbox{RMSE}_{\mu} = \sqrt{\frac{1}{N}\sum_{u,i}^{N}(\mu-y_{u,i})^2} = \sigma
$$

### Movie effect model

We can improve our model by adding a term, $b_i$, that represents the effect of each movie's average rating:

$$
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
$$

Here, $b_i$ is approximately the average of $Y_{u,i}$ minus the overall mean for each movie $i$. We can again use least squares to estimate the $b_i$ in the following way:

```{r lm_function, eval=FALSE}
fit <- lm(rating ~ as.factor(movieId), data=edx)
```

Note that because there are thousands of b's, the `lm()` function will be very slow or cause R to crash, so it is not recommendable using linear regression to calculate these effects. So we will calculate all $b_i$'s, and subsequently the predicted ratings, in the way shown below.

```{r movie_effects}
movie_avgs <- train %>% group_by(movieId) %>% summarize(b_i = mean(rating-mu))

predicted_ratings <- mu + test %>% left_join(movie_avgs, by='movieId') %>%
  pull(b_i)

summary(predicted_ratings)
```

From the summary above, we can see that the predictions obtained range from 0.5 and 4.75, inside the real range (0.5 to 5). Calculating `RMSE(test$rating, predicted_ratings)`, we get the results shown in the next table. 

```{r movie_effects_results, echo=FALSE}
rmse_results <- bind_rows(rmse_results,
                          tibble(Method="Movie effect model",  
                                 RMSE = RMSE(test$rating, predicted_ratings)))
kbl(rmse_results, booktabs = TRUE, caption = 'RMSE results with the Movie effect model') %>%  kable_styling(latex_options = c("striped","hold_position"))
```

A 12% drop is a big improvement, but we can still do better. 

### Movie + User effects model

We further improve our model by adding $b_u$, the user-specific effect:

$$ 
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
$$

To fit this model, we could again calculate the least squares estimate by using `lm` like this:

```{r lm_movie_user, eval=FALSE}
lm(rating ~ as.factor(movieId) + as.factor(userId))

```

But for the same reason explained before, we will follow a simpler model approach, with $b_u$ being calculated with the difference between $Y_{u,i}$ and both overall mean rating and movie-specific coefficient. It is demonstrated in the code chunk below: 

```{r m_u_effects}
user_avgs <- train %>%  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%  summarize(b_u = mean(rating - mu - b_i))

predicted_ratings <- test %>%
  left_join(movie_avgs, by='movieId') %>%  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%  pull(pred)

summary(predicted_ratings)
```

Predicted ratings now have a spread from -0.7161 to  6.0547, which is bigger than the real range (0.5 to 5). There are `r sum(predicted_ratings<0.5)` values under 0.5 and `r sum(predicted_ratings>5)` over 5. But what we really need to evaluate is the RMSE value by applying the RMSE function.

```{r m_u_effects_results, echo=FALSE}
rmse_results <- bind_rows(rmse_results,
                          tibble(Method="Movie + User effects model",  
                                 RMSE = RMSE(test$rating, predicted_ratings)))
kbl(rmse_results, booktabs = TRUE, caption = 'RMSE results with the Movie and User effects model') %>%  kable_styling(latex_options = c("striped","hold_position"))

rm(predicted_ratings)
```

Another big improvement in the RMSE value with a 8% drop. Let's proceed with other models.

### Movie + User + Genre effects model

Now we try an approach proposed by Prof. Irrizarry, which was presented in his book but not built and developed in it, that adds the genre effect to the model. Its formula is presented as shown below:

$$
Y_{u,i} = \mu + b_i + b_u + \sum_{k=1}^K x^k_{u,i} \beta_k + \varepsilon_{u,i}
$$

with $x^k_{u,i} = 1$ if $g_{u,i}$ is genre $k$. We already determined $x_{u,i}$ values earlier by creating binary columns for each of the 20 genres available in the training and test data. Now we determine $\beta_k$ values with the same approach utilized in the last two models, where each $\beta_k$ is the mean of the subtraction of overall average rating, movie-specific coefficient $b_i$ and user-specific coefficient $b_u$ from ratings $Y_{u,i}$. This way, if a movie belongs to both Comedy and Drama, one $\beta$ will throw the average rating up (high ratings for Drama) and the other down (low average for Comedies).

To make all the process faster down the line, we will first add $b_i$ and  $b_u$ columns to the training and test sets, and then we calculate all $\beta_k$'s.

```{r beta_genre}
train <- train %>% left_join(movie_avgs, by='movieId') %>% 
  left_join(user_avgs, by='userId') 
test <- test %>% left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId')
rm(user_avgs,movie_avgs)

beta_k <- vector()
for (i in seq_along(all_genres)) {
  b_value<- train %>% 
    group_by(!!sym(all_genres[[i]])) %>%          
    summarize(beta_k=mean(rating-mu-b_i-b_u)) %>% 
    filter((.[1])==1) %>% .[[2]]                  
  beta_k <- append(beta_k, b_value)               
}
rm(i,b_value)

df_beta<-data.frame(beta_k) 
rownames(df_beta) <- all_genres
```

First, we group the training set by each genre column by unquoting their names with argument `!!` and function `sym`, which forms two different groups (genre = 0 and genre = 1). We then calculate each specific genre's beta-values, and filter for column values equal to 1 (the genre is present). We append it to the `beta_k` vector and, lastly, transforme it into a dataframe, as shown below.

```{r df_beta_divided, echo=FALSE}
df_beta_div <-data.frame(row.names(df_beta)[1:10], df_beta[1:10,],
                         row.names(df_beta)[11:20], df_beta[11:20,]) # Dividing table in two for better placement in report
colnames(df_beta_div) <- c('genre','beta_k','genre','beta_k')
df_beta_div %>% kbl(booktabs = TRUE, linesep = "", caption = 'Genre-specific beta values') %>% column_spec(1:4, width='8em') %>% kable_styling(latex_options = c("striped","hold_position")) %>% add_header_above(c("df_beta[1:10]" = 2, "df_beta[11:20]" = 2))
```

The sum within the model's prediction equation is represented below:

$$
\sum_{k=1}^K x_{u,i}\beta_{k} = x^{\{1\}}_{u,i}\beta_{1} +  x^{\{2\}}_{u,i}\beta_{2} + ... +  x^{\{20\}}_{u,i}\beta_{20}
$$

We apply this sum by performing a matrix multiplication between the test set genre columns (`test[,4:23]`) and all 20 $\beta_k$ values calculated. This multiplication, between a `r dim(test)[1]` per 20 matrix and a 20 per 1 vector, results in a `r dim(test)[1]` per 1 vector. We then add this as a column to the test set, so we can calculate the predictions.

```{r m_u_g_effects, warning=FALSE}
sum_x_beta <- as.matrix(test[,4:23])%*%as.matrix(df_beta) 
sum_x_beta <- data.frame(sum_x_beta=sum_x_beta[,1])       

test <- data.frame(test, sum_x_beta)
rm(df_beta,beta_k,sum_x_beta,all_genres)

predicted_ratings <- test %>%
  mutate(pred = mu + b_i + b_u + sum_x_beta) %>%
  pull(pred)

summary(predicted_ratings)
```

Analyzing the values obtained for ratings, we see there are `r sum(predicted_ratings<0.5)` values under 0.5 and `r sum(predicted_ratings>5)` predicted ratings over 5.

```{r m_u_g_effects_results, echo=FALSE}
rmse_results <- bind_rows(rmse_results,
                          tibble(Method="Movie + User + Genre effects model",  
                                 RMSE = RMSE(test$rating, predicted_ratings)))
kbl(rmse_results, booktabs = TRUE, caption = 'RMSE results with the Movie, User and Genre effects model') %>%  kable_styling(latex_options = c("striped","hold_position"))
rm(predicted_ratings)
```

While the work needed to obtain this prediction model was arduous, there was very little improvement from the last model.

### Regularized Movie + User effects model

When a movie receives ratings from just a few users or when users give only very few reviews, we have more uncertainty. Therefore, larger estimates of both $b_i$ and $b_u$, negative or positive, are more likely. Consider a case in which we have a movie with 100 user ratings and another movie with just one. While the first movie's $b_i$ will be pretty accurate, the estimate for the second movie will simply be the observed deviation from the average rating, which is a clear sign of overtraining. In this cases, it is better to make this prediction with just the overall average rating $\mu$. That is why we need a form of penalization. Regularization permits us to penalize large estimates that are formed using these small sample sizes.

This time, we will add regularization to one of the previously tried models. Since adding genre didn't have much effect on RMSE and both optimal $\lambda$ determination (which we'll perform in this section) and $\beta_k$ calculations are computationally costly, we will use the Movie + User effects model. So now, instead of minimizing the least squares equation, we minimize an equation that adds a penalty. We need to apply regularization to both estimate user and movie effects. We are minimizing:

$$
\sum_{u,i} \left(y_{u,i} - \mu - b_i - b_u \right)^2 + 
\lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2\right)
$$

The formula below shows the new form of calculating $b_i$, with the inclusion of $\lambda$ in the mean values calculation. When our sample size $n_i$ is very large, a case which will give us a stable estimate, then the penalty $\lambda$ is effectively ignored since $n_i+\lambda\approx n_i$. However, when the $n_i$ is small, then the estimate  $\hat{b}_i(\lambda)$ is shrunken towards 0. The larger $\lambda$, the more we shrink.

$$
\hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)
$$

In the same way, $\hat{b}_u(\lambda)$ is the regularized $b_i$ by including $\lambda$ in the division denominator.

$$
\hat{b}_u(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{b}_{i} - \hat{\mu}\right)
$$

We must first calculate various predictions and their respective RMSEs by testing $\lambda$ values varying from 0 to 10, with a 0.25 stepsize, to determine which regularization value gives us the lowest RMSE.

```{r regularized_m_u_effects, fig.height=3, fig.fullwidth=TRUE , fig.align='center', fig.cap='RMSE values per lambda'}
train <- train %>% select(userId, movieId, rating)
test <- test %>% select(userId, movieId, rating)

lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  b_i <- train %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))       
  b_u <- train %>%
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l)) 
  predicted_ratings <-
    test %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)                                      
  return(RMSE(test$rating, predicted_ratings))
})
qplot(lambdas, rmses, xlab='lambda', ylab='RMSE')
```

By analyzing the plot and retrieving values from the table generated, we can confirm that the optimal $\lambda$ value is `r lambdas[which.min(rmses)]`, and its corresponding RMSE is equal to `r min(rmses)`.

```{r optimal_trained_model, echo=FALSE}
b_i <- train %>% group_by(movieId) %>%
summarize(b_i = sum(rating - mu)/(n()+5))

b_u <- train %>% left_join(b_i, by="movieId") %>% group_by(userId) %>%
summarize(b_u = sum(rating - b_i - mu)/(n()+5)) 

predicted_ratings <- test %>%
left_join(b_i, by = "movieId") %>%
left_join(b_u, by = "userId") %>%
mutate(pred = mu + b_i + b_u) %>%
pull(pred)                                      

summary(predicted_ratings)
```

As usual, the `summary` function shows us that there are predicted values under 0.5 (`r sum(predicted_ratings<0.5)` predictions) and  over 5.0 (`r sum(predicted_ratings>5)`).

```{r lambda_rmse_results, echo=FALSE}
rmse_results <- bind_rows(
  rmse_results,
  tibble(Method="Regularized Movie + User effects model",  
         RMSE = min(rmses)))
kbl(rmse_results, booktabs = TRUE, caption = 'RMSE results with the Regularized Movie and User effects model') %>%
  kable_styling(latex_options = c("striped","HOLD_position"))

rm(lambdas,rmses,mu) # Cleaning variables to recover RAM space
```

Once more there is improvement, but not by much.

### Matrix Factorization with _recosystem_ package

[Recosystem introduction page](https://cran.r-project.org/web/packages/recosystem/vignettes/introduction.html)

The approach we were taking was not getting much better, so we try a different one: matrix factorization. It is a class of collaborative filtering algorithms used in recommendation systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices.This family of methods became widely known during the Netflix prize challenge due to its effectiveness. Prediction results can be improved by assigning different regularization weights to the latent factors, based on movies' popularity and users' activeness.

Groups of movies, as well as groups of users, have similar rating patterns. By looking at the correlation between movies, we can see rating patterns, and this is basically what matrix factorization does. It is typically used to approximate an incomplete matrix using the product of two matrices in a latent space.
This incomplete matrix, $R_{m×n}$, is a matrix where rows represent different users and columns represent movies, each cell inside the matrix representing a rating (or the absence of one).

The idea of matrix factorization is to approximate the whole rating matrix $R_{m×n}$ by the product of two matrices of lower dimensions, $P_{k×m}$ and $Q_{k×n}$, such that

$$
R \approx P'Q
$$

Let $p_u$ be the $u$-th column of $P$, and $q_i$ be the $i$-th column of $Q$, then the rating given by user $u$ on item $i$ would be predicted as $p_u'q_i$. A typical solution for P and Q is the following:


$$
\min_{P,Q} \sum_{(u,i)∈R}[f(p_u,q_i;r_{u,i})+\mu_P||p_u||_1+\mu_Q||q_i||_1+\frac{\lambda_P}{2}||pu||^2_2+\frac{λ_Q}{2}||q_i||_2^2]
$$

where $(u,i)$ are locations of observed entries in $R$, $r_{u,i}$ is the observed rating, $f$ is the loss function, and $μ_P$,$μ_Q$,$λ_P$,$λ_Q$ are penalty parameters to avoid overfitting.

The _recosystem_ package easily performs the factorization. But to do it, we need to input a parse matrix in triplet form, i.e., each line in the file contains three numbers: `user_index`, `item_index` and `rating`. User and item indexes may start with either 0 or 1, and this can be specified by the `index1` parameter in `data_memory()`. To train and predict, we need to provide objects of class `DataSource`, such as the previously mentioned `data_memory()`, which saves a dataset as an R objects.

```{r recosystem_training, warning=FALSE, message=FALSE, results='hide'}
train_dm <- data_memory(user_index = train$userId, item_index = train$movieId,
                          rating = train$rating, index1 = TRUE) 
rm(train)

test_dm <- data_memory(user_index = test$userId, item_index = test$movieId, 
                       index1 = TRUE)                           
test_rating <- test$rating                                      
rm(test)

set.seed(123, sample.kind = "Rounding")
r <- Reco()                            
params = r$tune(train_dm, opts = list(dim = c(15, 20),
                                        costp_l1 = 0,
                                        costp_l2 = c(0.01, 0.1),
                                        costq_l1 = 0,
                                        costq_l2 = c(0.01, 0.1),
                                        lrate = c(0.075, 0.1), nthread = 2))

optimal_params = params$min

r$train(train_dm, opts = c(optimal_params, nthread = 1, niter = 20))
```

`r optimal_params[[1]]`, `r optimal_params[[2]]`

with the trained model, it is possible to predict the ratings for the test set.

```{r predicting_MF, message=FALSE}
predicted_ratings = r$predict(test_dm, out_memory()) 

summary(predicted_ratings)
```

object of Output class
out_memory(): Result should be returned as R objects

`r sum(predicted_ratings<0.5)` and `r sum(predicted_ratings>5)`.

```{r MF_results, echo=FALSE}
RMSE(test_rating, predicted_ratings)

rmse_results <- bind_rows(rmse_results,
                          tibble(Method="Matrix Factorization Model",  
                                 RMSE = RMSE(test_rating, predicted_ratings)))

kbl(rmse_results, booktabs = TRUE, linesep = "", caption = 'RMSE results with the Matrix Factorization Model') %>%  
  kable_styling(latex_options = c("striped","hold_position"))

rm(test_rating,predicted_ratings,params,test_dm)
```

# Results

## Training full edx dataset with Matrix Factorization

To guarantee the prediction model can be applied to the whole validation set, we will train with full `edx` set.

```{r recosystem_edx, message=FALSE, results='hide'}
train_dm <- data_memory(user_index = edx$userId, item_index = edx$movieId, 
                          rating = edx$rating, index1 = TRUE)
rm(edx)

r$train(train_dm, opts = c(optimal_params, nthread = 1, niter = 20))
```

## Validation set's predicted ratings and RMSES

predicting for validation dataset, we get the following values:

```{r prediction_edx, message=FALSE}
validation_dm <- data_memory(user_index = validation$userId,
                             item_index = validation$movieId, index1 = TRUE)
validation_rating <- validation$rating

predicted_ratings = r$predict(validation_dm, out_memory()) 
rm(train_dm,validation)

summary(predicted_ratings)
```

`r sum(predicted_ratings<0.5)` and `r sum(predicted_ratings>5)`.

```{r results_validation, echo=FALSE}
RMSE(validation_rating, predicted_ratings)

rmse_validation <- tibble(Method="Matrix Factorization Model",  
                                 RMSE = RMSE(validation_rating, predicted_ratings))
colnames(rmse_validation) <- c('Method','Validation RMSE')
kbl(rmse_validation, booktabs = TRUE, caption = 'RMSE result for the validation set with Matrix Factorization Model') %>%  kable_styling(latex_options = c("striped","hold_position"))
```


# Conclusion

We visualized  the importance of variability between different movies and users, the genre  and genre combinations effect on ratings, some not that significant. A lot of models were evaluated, the genre addition didn't have a great effect on the model so we didn't utilize it during the regularized trial, and the best and easiest model to apply was Matrix Factorization with _recosystem_.


