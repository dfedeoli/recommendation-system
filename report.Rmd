---
title: "Recommendation Systems: Rating Predictions with _MovieLens_ Data"
author: "Danilo Ferreira de Oliveira"
date: "04/22/2021"
output: 
  pdf_document:
    citation_package: natbib
    highlight: tango
    latex_engine: xelatex
    number_sections: true
    toc: true
    toc_depth: 3
bibliography: bibliography.bib
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = '>>', tidy = TRUE) #, fig.pos = "!H", out.extra = "") 
```

# Introduction

Companies like _Amazon_, _YouTube_ or _Netflix_ possess massive user ratings datasets, which they were able to collect from thousands of products sold to or used by their customers. Recommendation systems use those ratings to make user-specific recommendations. With regard to movies, we can predict what rating a user  would give movie pictures they haven't seen by analyzing the ones he or she likes to watch, and then recommend those with relatively high predicted ratings. These are more complicated machine learning challenges because each outcome has a different set of predictors, seeing that different users rate different movies and different quantities of movies.

_Netflix_ used to have a stars rating system (which now changed to a like-dislike system) and its recommendation algorithm would predict how many stars a particular user would give a specific movie. One star suggested the movie in question was not good, whereas five stars suggested it was an excellent one. In October 2006, _Netflix_ proposed the data science community a challenge which was to improve their algorithm by 10%. Inspired by that challenge, we will develop the foundations to a movie recommendation system. Since _Netflix_ data is not publicly available, we will use the _GroupLens_ research lab database, which you can find [here](https://grouplens.org/datasets/movielens/). We will be creating our own recommendation system using tools shown throughout all courses in the _HarvardX: Data Science Professional Certificate_ series, including _R Language_ for statistical computing and its most famous IDE, _RStudio_.

There are various datasets available in the _MovieLens_ website, but we will use the [10M version one](https://grouplens.org/datasets/movielens/10m/) to make computations a little easier. Released in January 2009, it is a stable benchmark dataset, and provides approximately 10 million ratings applied to near 10,000 movies by 72,000 users.

We will initially run a _R_ script provided by _HarvardX_ to generate our datasets. We continue by analyzing and visualizing the obtained training dataframe, and by developing rating prediction algorithms. With root mean squared error (RMSE) as our loss function, we select the best algorithm and, for its final test, we predict movie ratings in the validation set (the final hold-out test set) as if they were unknown. RMSE will again be used to evaluate how close your predictions are to the true values in the final hold-out test set.

While training machine learning models, we will first follow the approach made by @rafalab and further develop it with the addition of genre effects. We will also train a matrix factorization model.

# Analysis

## Gathering and structuring datasets

```{r install_packages, echo=FALSE, warning=FALSE,message=FALSE}
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(rlist)) install.packages("rlist", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(recosystem)) install.packages("recosystem", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(tibble)) install.packages("tibble", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(cowplot)) install.packages("cowplot", repos = "http://cran.us.r-project.org")
```

 Before we can start gathering our data and generating the different sets, we load the necessary R libraries for the whole report's development.

```{r libraries, warning=FALSE,message=FALSE}
library(caret)     
library(tidyverse) 
library(ggplot2)
library(lubridate)
library(stringr)
library(recosystem)
library(data.table)
library(kableExtra)
library(tibble)
```

Next, we download the _MovieLens_ data and run code provided by _edX_ to generate the datasets for training and evaluating the final model. The `edx` set will be used all along the report to train and choose the best model for rating predictions, while the `validation` set will only be used in the end to evaluate the quality of the final model with a root mean square error function. The provided script begins by downloading files from the _GroupLens_ database and performing transformations in order to obtain the `movielens` dataframe below.

```{r getting_data, eval=FALSE}
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")
```

The validation set will be 10% of `movielens`. We need to make sure all user and movie IDs in `validation` are also in `edx`, so we apply `semi_join()` transformations. Then, we add rows removed from the validation set back into the training set.

```{r creating_edx_validation, warning=FALSE, message=FALSE, results='hide', eval=FALSE}
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

We can now start analyzing the `edx` dataframe.

```{r load_edx_validation, echo=FALSE}
load('rda/edx.rda')
load('rda/validation.rda')
```


## Exploratory data analysis

To begin, let's take a look at our dataset's columns and variables. 

```{r head_of_edx}
edx %>% as_tibble() 
```

It is observed that columns `title` and `genres` are of class "character" ($<$chr$>$), while `movieId` and `rating` are numeric ($<$dbl$>$), `userId`, `timestamp` are integers ($<$int$>$). We should also analyze the number of unique values for each important feature column. 

```{r summary_n_distinct}
edx %>% summarize(n_userIds = n_distinct(userId), n_movieIds = n_distinct(movieId),
                  n_titles = n_distinct(title), n_genres = n_distinct(genres)) %>% 
  kbl(booktabs = TRUE,
      caption = "Distinct values for userId, movieId, titles and genres") %>% 
  kable_styling(latex_options = c("striped", "HOLD_position"))
```

Note, from Table 1, that there are more unique `movieId` variables than there are `title` ones. We need to investigate why.

```{r movies_with_multiple_Ids}
edx %>% group_by(title) %>% 
  summarize(n_movieIds = n_distinct(movieId)) %>% filter(n_movieIds > 1) %>% 
  kbl(booktabs = TRUE, caption = "Movies with more than one ID") %>% 
  kable_styling(latex_options = c("striped","HOLD_position"))
```

Table 2 shows that _War of the Worlds (2005)_ has two distinct `movieId` numbers. Let's see how many reviews each of them have.

```{r movieIds_for_WW2005, echo=FALSE}
edx %>% filter(title=='War of the Worlds (2005)') %>% 
  group_by(movieId) %>% summarize(title=title[1], genres=genres[1], n=n()) %>%  # Count of reviews for each movieId
  kbl(booktabs = TRUE, caption = "Different values of movieId and genres for \\textit{War of the Worlds (2005)}") %>% kable_styling(latex_options = c("striped","HOLD_position"))
```

Its second `movieId` received only 28 reviews (Table 3). This is a very low value, and since there is a possibility both IDs are in the validation set, we won't meddle with it.

### Ratings

We can now visualize the data. First, we see how the ratings distribution is configured in Figure 1, and from it we can affirm that full star ratings are more common than those with half stars, since $4$, $3$ and $5$ are the most frequent ones.

```{r ratings_distribution, echo=FALSE, fig.height=3, fig.fullwidth=TRUE, fig.align='center', fig.cap = "Distribution of ratings"}
edx %>% mutate(stars=ifelse(rating %in% c(1:5),'full','half')) %>% 
  ggplot(aes(factor(rating),fill=stars)) + geom_bar() + scale_fill_hue(c=80) + 
  labs(x='rating')
```

### Movies and users

Our dataset has 10,676 different movies and 69,878 different users. By multiplying these numbers, we can verify that not every user rated every movie. We can think of the dataset as a very large matrix that has users as its row variables, movies as its columns and ratings (or `NA` values) within its cells. The task of a recommendation system can be regarded as filling in the empty values in this matrix. Let's evaluate the frequency distribution of reviews that each user gives and each movie receives by plotting these values.

```{r hist_count, echo=FALSE, fig.align='center', fig.height=3, fig.cap= 'Frequency of reviews given by a specific user (left) and of reviews received by a specific movie (right)'}
p1 <- edx %>% count(userId) %>% ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + scale_x_log10() + 
  labs(x='reviews given by user') # Number of reviews from each user

p2 <- edx %>% count(movieId) %>% ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + scale_x_log10() +
  labs(x='reviews received by movie',y='') # Number of reviews of each movie

gridExtra::grid.arrange(p1, p2, ncol = 2)
rm(p1,p2)
```

The x-axis in the generated plots from Figure 2 are both on a logarithmic scale, and the one on the left is right-skewed, meaning its average value is higher than the mode. By inspecting them, we can affirm that the majority of users give 20 to 200 reviews, while each movie frequently receives between 20 and a 1000 reviews, the latter being a pretty broad range.

Plotting histograms for average ratings, we get Figure 3. From the top plot, it is possible to see that it is rare for a user to average rating movies below 2, since there are few users to the left of that number (only `r edx %>% group_by(userId) %>% summarize(avg=mean(rating)) %>% filter(avg< 2) %>% dim() %>% .[1]` of all `r edx %>% summarize(n_userIds = n_distinct(userId)) %>% .[1,1]` in the dataset, to be more exact). We can say the same about averaging over 4.5 (`r edx %>% group_by(userId) %>% summarize(avg=mean(rating)) %>% filter(avg>4.5) %>% dim() %>% .[1]` users). And from all 10,676 movies, we can also see that few of them average below 1.5 (only `r edx %>% group_by(movieId) %>% summarize(avg=mean(rating)) %>% filter(avg< 1.5) %>% dim() %>% .[1]`) and over 4.25 rating (`r edx %>% group_by(movieId) %>% summarize(avg=mean(rating)) %>% filter(avg> 4.25) %>% dim() %>% .[1]` films).

```{r hist_avg_rating, echo=FALSE, fig.align='center', fig.height=3, fig.cap='Frequency of average rating given by users (top) and average rating received by movies (bottom)'}
p1 <- edx %>% group_by(userId) %>% summarize(avg=mean(rating)) %>% 
  ggplot(aes(avg)) + geom_histogram(bins=35, color='black') +
  labs(x='user average ratings') #average rating a user gives

p2 <- edx %>% group_by(movieId) %>% summarize(avg=mean(rating)) %>% 
  ggplot(aes(avg)) + geom_histogram(bins=35, color='black') +
  labs(x='movie average ratings') # Average rating received by a movie

cowplot::plot_grid(p1,p2,ncol=1, align='v')
rm(p1,p2)
```

By exploring the top 10 most reviewed movies in the dataset (Figure 4) and how many times they were reviewed, we observe that movies that receive the most reviews are normally blockbusters.

```{r unique_movies, echo=FALSE, fig.height=3.5, fig.fullwidth=TRUE, fig.align='center', fig.cap='Number of reviews for the top 10 most reviewed movies'}
unique_movies <- edx %>% group_by(title) %>% 
  summarize(title=title[1], genres=genres[1], count=n()) %>% arrange(desc(count))
unique_movies[1:10,] %>% mutate(title = str_remove(title, ", The"),
                                title = str_remove(title, " - A New Hope \\(a.k.a. Star Wars\\)")) %>% 
  ggplot(aes(x=reorder(title, count), y=count)) + geom_bar(stat = "identity") + labs(x='') + coord_flip()
```

Checking for their average ratings in Figure 5, we also notice they have really high values, the lowest of them being 3.66. We can infer that the more a movie is reviewed, the better its rating is. It makes sense, given that the better the movie, the higher its ratings and the more people go watch it, subsequently more users can rate it.

```{r movies_ratings, echo=FALSE, fig.height=3.5, fig.fullwidth=TRUE, fig.align='center', fig.cap='Average rating for the top 10 most reviewed movies'}
edx %>% filter(title %in% head(unique_movies$title,n=10)) %>% group_by(title) %>% 
  summarize(avg=mean(rating), count=n()) %>% mutate(title = str_remove(title, ", The"),
                                                                title = str_remove(title, " - A New Hope \\(a.k.a. Star Wars\\)")) %>% 
  ggplot(aes(x=reorder(title, count), y=avg)) + geom_bar(stat = "identity") + labs(x='',y='average rating') + coord_flip()
```


### Release date

Since there isn't a column for movie's release date, we will check it by using the `str_extract` function from the `stringr` package, as the `title` column includes each movie's release date inside parenthesis.

```{r release_date}
release_date <- edx$title %>% str_extract('\\([0-9]{4}\\)') %>%
  str_extract('[0-9]{4}') %>% as.integer()
summary(release_date)
```

We first apply this function extracting the date along with the parenthesis, and then the number from inside them. We do it like this because, by extracting only the number, movies like _2001: A Space Odyssey (1968)_ and _2001 Maniacs (2005)_ show up when searching for year 2001. To go even further, it also extracts the numbers 1000 and 9000 for _House of 1000 Corpses (2003)_ and _Detroit 9000 (1973)_, respectively.

As an example, we see in Table 4 that the earliest release date in our dataset (1915) only has one movie (and a rather controversial one):

```{r released_edx, echo=FALSE}
edx <- data.frame(edx,released=release_date)
edx[which(edx$released==1915),] %>% summarize(movieId=movieId[1], title=title[1], genres=genres[1], n=n()) %>% kbl(booktabs = TRUE, linesep = "", caption = "Movie with the earliest release date in the dataset") %>% kable_styling(latex_options = c("striped","HOLD_position"))
```

We now compute the number of ratings for each movie and plot it against the year the movie came out, using a boxplot for each year (Figure 6). A logarithmic transformation is applied on the y-axis (number of ratings) when creating the plot.

```{r released_count, echo=FALSE, fig.fullwidth=TRUE, fig.align='center', fig.height=4, fig.cap="Number of reviews the movies received versus the year they were released"}
edx %>% group_by(movieId) %>%
  summarize(n = n(), released = as.character(first(released))) %>%
  ggplot(aes(released, n)) + geom_boxplot(fill= 'orange', color='#808080', alpha=0.2) +
  scale_y_log10(breaks=c(1,10,100,200,500,1000,10000)) + scale_x_discrete(breaks=seq(1915,2005,5)) + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) + labs(x='year released',y='count')
```

We see that, on average, movies that came out in the mid 90's get more ratings, reaching values over 30,000 reviews. We also see that with newer movies, starting near 1998, the number of ratings decreases with year: the more recent a movie is, the less time users have had to rate it.

Similarly to before, we compute the average of ratings for each movie and plot it against the release year with boxplots, as shown in Figure 7.

```{r released_avg_rating, echo=FALSE, fig.fullwidth=TRUE, fig.align='center', fig.height=4, fig.cap="Average rating the movies received versus the year they were released"}
edx %>% group_by(movieId) %>%
  summarize(n = n(), released = as.character(first(released)), avg_rating=mean(rating)) %>%
  ggplot(aes(released, avg_rating)) + geom_boxplot(fill= 'orange', color='#808080', alpha=0.2) + scale_x_discrete(breaks=seq(1915,2005,5)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) + labs(x='year released', y='average rating')
edx <- edx[,-ncol(edx),drop=FALSE]
rm(release_date)
```

It is possible to see that most movies released until 1973 normally range between 3.0 and 4.0 ratings. From then on, the range gets bigger and values get lower (median values go from averaging over 3.5 to near 3.25). There are even movies from around the 2000's that average 0.5 ratings, but they are outliers and probably only received very few ratings.

### Genres and genre combinations

The `genres` variable actually represents combinations of a series of unique genres. We will count the number of reviews each different combination that appear in our dataset has and arrange them in descending order. Table 5 shows, as an example, the top 7 most reviewed genre combinations.

```{r genre_combinations, echo=FALSE}
edx %>% group_by(genres) %>% summarize(count = n()) %>% arrange(desc(count)) %>% head(n=7) %>% 
  data.frame(rank=c(1:7), .) %>% kbl(booktabs = TRUE, linesep = "", caption = 'Top genre combinations rated') %>% kable_styling(latex_options = c("striped","HOLD_position"))
```

Let's check the genre effect on ratings. We will filter for genre combinations that were reviewed over a thousand times, put them in ascending order of average rating and visualize 15 examples, equally distant inside the arranged dataframe. The indexes to select the combinations in the arranged set are represented by the variable `gcomb_15`.

```{r genre_effect, fig.fullwidth=TRUE, fig.align='center', fig.height=4, fig.cap="Distribution for selected genre combinations' average ratings"}
gcomb_15 <- c(1,seq(31,415,32),444) 
edx %>% group_by(genres) %>%
  summarize(n = n(), avg = mean(rating), se = sd(rating)/sqrt(n())) %>%     
  filter(n > 1000) %>% arrange(desc(avg)) %>%  .[gcomb_15,] %>%
  mutate(genres = reorder(genres, avg)) %>%
  ggplot(aes(x = genres, y = avg,
             ymin = avg - 2*se, ymax = avg + 2*se)) + geom_point() + 
  geom_errorbar() + theme(axis.text.x=element_text(angle = 30, hjust = 1)) + 
  labs(x='', y= 'average rating')
```

Inspecting the plot from Figure 8, we see the clear effect that genre combinations have on ratings, with average values ranging from approximately 2.0 to over 4.5. We can also investigate the effect of each genre that composes these combinations separately. There are 20 different genres in this dataset, and they are shown in Table 6:

```{r all_genres, echo=FALSE}
rm(gcomb_15)
genre_combinations <- unique(edx$genres)     # Unique genre combinations
all_genres <- character()                    # Empty character vector
for (i in genre_combinations) {
  movie_genres <- str_split(i,'\\|')[[1]]    # Splitting the genre combinations
  for (j in movie_genres) {
    all_genres <- rlist::list.append(all_genres, j) # Appending each genre
  }
}
all_genres <- unique(all_genres)             # Unique genres in the dataset
rm(genre_combinations,movie_genres,i,j)
matrix(all_genres, nrow = 5, ncol= 4) %>% kbl(booktabs = TRUE, linesep = "", caption = 'All unique genres present in the dataset') %>% column_spec(1:4, width='8em') %>% kable_styling(latex_options =c("striped","HOLD_position"))
```

We can count how many reviews each genre have had by applying the `sapply()` function:

```{r all_genres_count}
all_genres_count <- sapply(all_genres, function(g) {
  sum(str_detect(edx$genres, g))      
}) %>% data.frame(review_count=.) %>% 
  arrange(desc(.))

```

The results are displayed on Table 7. The number of reviews per genre range from near 4 million to only 7.    

```{r all_genres_count_div, echo=FALSE}
all_genres_count_div <-data.frame(row.names(all_genres_count)[1:10], all_genres_count[1:10,],
                                  row.names(all_genres_count)[11:20], all_genres_count[11:20,]) # Dividing table in two for better placement in report
colnames(all_genres_count_div) <- c('Genre','Count','Genre','Count')
all_genres_count_div %>% kbl(booktabs = TRUE, linesep = "", caption = 'Number of reviews per genre') %>% column_spec(1:4, width='9em') %>% kable_styling(latex_options =c("striped","HOLD_position")) %>% add_header_above(c("all_genres_count[1:10]" = 2, "all_genres_count[11:20]" = 2)) # all_genres_count[1:10]
 
```

We must investigate the `(no genres listed)` genre, which actually represents no genre at all. We can see all movies that belong to it and their respective averages and standard errors:

```{r no_genres_listed}
edx[which(edx$genres=='(no genres listed)'),] %>% 
  summarize(movieId=movieId[1], userId=userId[1], title=title[1], genres=genres[1],
            n = n(), avg = mean(rating), se = sd(rating)/sqrt(n()))
```

There is only one movie in the dataset with no genres listed and it has only 7 reviews, giving its average rating a high standard error (0.4185). For this reason, we won't be plotting this genre's average rating along with the others in Figure 9. The forementioned plot shows us that average genre ratings range from under 3.3 to over 4.0, clarifying even further the effect genres have on ratings.

```{r all_genres_avg_rating, echo=FALSE, fig.align='center', fig.width=6, fig.length=3.5, fig.cap='Average rating for each genre'}
all_genres_avg <- matrix(nrow=20,ncol=2)
i <- 1
for (var in all_genres) {
  vals <- edx[which(str_detect(edx$genres,as.character(var))),] %>% 
    summarize(n = n(), avg = mean(rating), se = sd(rating)/sqrt(n()))
  all_genres_avg[i,1] <- vals$avg
  all_genres_avg[i,2] <- vals$se
  i <- i+1
}
data.frame(genres=all_genres, avg=all_genres_avg[,1], se=all_genres_avg[,2]) %>% .[1:19,] %>%
  mutate(genres = reorder(genres, avg)) %>%
  ggplot(aes(x = genres, y = avg, ymin = avg - 2*se, ymax = avg + 2*se)) +  # Plotting the values and errors
  geom_point() + geom_errorbar() + theme(axis.text.x=element_text(angle = 45, hjust = 1)) + 
  labs(x='', y= 'average rating')
rm(var,i,vals,all_genres_avg)
```

We now observe the top genres reviewed in this dataset and how many reviews each one of them have. We accounted for genres that were reviewed over a million times, resulting in 8 distinct possibilities. The complete genre reviews count has already been shown in the form of Table 7, but we will plot the top 8 most reviewed genres (Fig. 10) to better visualize them and to also compare their shape with their respective movies count plot (Fig. 11). 

```{r top_genres_reviews, echo=FALSE, fig.width=6, fig.height=3, fig.fullwidth=TRUE, fig.align='center', fig.cap='Number of reviews for each of the top 8 most reviewed genres'}
top_genres <- all_genres_count %>% filter(.>=1e6) # Genres with over 1 million reviews

top_genres %>% ggplot(aes(x= row.names(.), y = review_count, fill=row.names(.))) +
  geom_bar(stat='identity') + scale_fill_hue(c=40) + theme(legend.position="none") + 
  labs(x='', y='reviews count')
```

```{r movies_top_genre, echo=FALSE, fig.height=3, fig.fullwidth=TRUE, fig.align='center', fig.cap='Number of movies that belong to the top 8 most reviewed genres'}
movies_per_top_genre <- sapply(row.names(top_genres), function(g) {
  sum(str_detect(unique_movies$genres, g))
})
data.frame(movies_per_top_genre) %>% 
  ggplot(aes(x= row.names(.), y = movies_per_top_genre, fill=row.names(.))) +
  geom_bar(stat='identity') + scale_fill_hue(c=40) + theme(legend.position="none") +
  labs(x='', y='movies count')
rm(movies_per_top_genre,top_genres,unique_movies,all_genres_count)
```

Generally, the barplot's shapes and proportions look pretty much the same for both review (Fig. 10) and movie counts (Fig. 11), with the exception of $Adventure$ and $Action$ having less movies than $Thriller$, even though they have more reviews. $Romance$ and $Thriller$ change positions from one plot to the other, but they are very close in proportion. Meanwhile, $Comedy$ and $Drama$ dominate in both counts.

### Rating date

Since the `timestamp` column doesn't provide us much value as it is, we create another column that gives us the date and time a movie was rated.

```{r timestamp_to_date}
edx <- mutate(edx, date = as_datetime(timestamp))
edx %>% as_tibble()
```

The new column `date` is of a Date-Time class (`<dttm>`) called "POSIXct", and it makes it better to understand the time of rating. Now we can plot the time effect on the dataset, showing the possible influence time (in which a movie was rated) has on the average value. Along with average ratings, Figure 12 presents a _LOESS_ smooth line that better shows the relationship between variables.

```{r time_effect, message=FALSE, fig.height=4, fig.fullwidth=TRUE, fig.align='center', fig.cap='Review date effect on the average rating'}
edx %>% mutate(date = round_date(date, unit = "week")) %>%
  group_by(date) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(date, rating)) +
  geom_point() + labs(y='average rating') +
  geom_smooth()
```

The review date definitely has some effect on ratings, but it isn't significant, so we will not be adding this effect in modeling.

## Feature engineering

Down the line, we will need all genres available in the dataset as binary values, so it is necessary to create columns for all 20 of them. For each row, we attribute value 1 to new columns that are present inside its `genres` cell, and value 0 to the rest. This means: if a determinate row has its `genres` value equal to `Comedy|Drama`, the newly created columns `Comedy` and `Drama` will have value 1, while the rest will remain zero. Here we see the first 6 rows, with only `movieId`, `title` and `genres`, so we can compare the genres further in this analysis.

```{r binary_genres_pre_creation, tidy=TRUE}
edx[,c(2,5,6)] %>% as_tibble(.rows = 6)
```

We apply the transformation by doing the following `for()` command. We will also permanently remove columns such as `timestamp`, `title`, `genres` and `date`, since they won't be needed to train the prediction models. 

```{r binary_genres_columns, tidy=TRUE}
for (var in all_genres) {
  edx <- edx %>% mutate(genre=ifelse(str_detect(genres,as.character(var)),1,0))
  colnames(edx)[ncol(edx)] <- as.character(var)
}
rm(var)
edx <- edx[,-(4:7),drop=FALSE]
edx[,4:23] %>% as_tibble(.rows=6)
```

Comparing each row in this tibble above with the ones demonstrated earlier, it is possible to see that the genres in the first one are now represented as $1$'s, while genres that don't appear before are kept as zero values. Therefore, the transformation occurred accurately.

## Creating training and test sets

Before training any model, we need to create a train and a test set. We first set the seed to guarantee the reproducibility of the model, and we divide the `edx` dataframe into 80-20% partitions.

```{r partitioning, warning=FALSE}
set.seed(123, sample.kind = "Rounding")
index <- createDataPartition(edx$rating, times=1, p=0.2, list= FALSE)
test <- edx[index,]
train <- edx[-index,]
```

```{r cleaning_index_edx, echo=FALSE}
rm(index)
edx <- edx %>% select(userId, movieId, rating)
```

To make sure all user and movie IDs in `test` are also in `train`, we apply `semi_join()` functions, just as we did for the first two datasets in the beginning of the report.

```{r test_semi_join}
test <- test %>% 
  semi_join(train, by = "movieId") %>%
  semi_join(train, by = "userId")
```

## Prediction models

Now that we have our training and test sets, we can start evaluating prediction models. We will begin by following approaches proposed by @rafalab, starting with a Overall Average Rating model, going through Movie, Movie + User and Regularized Movie + User effects models. We will also apply a method that includes Genre effect and one that performs Matrix Factorization. To evaluate their quality, we will use the RMSE function.

 If N is the number of user-movie combinations, $y_{u,i}$ is the rating for movie i by user u, and $\hat{y}_{u,i}$ is our prediction, then RMSE is defined as follows: 

$$
\mbox{RMSE} = \sqrt{\frac{1}{N} \sum_{u,i}^{N} \left( \hat{y}_{u,i} - y_{u,i} \right)^2 }
$$
which is represented by the function:

```{r rmse_function}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

### Overall Average Rating

We start with a model that assumes the same rating for all movies and all users, with all the differences explained by random variation: If $\mu$ represents the true rating for all movies and users and $\varepsilon$ represents independent errors sampled from the same distribution centered at zero, then:

$$
Y_{u,i} = \mu + \varepsilon_{u,i}
$$

In this case, the least squares estimate of $\mu$ (the estimate that minimizes the root mean squared error) is the average rating of all movies across all users [@rafalab]. So we predict all ratings as the mean rating of the training set and compare it to the actual ratings in the test set (`RMSE(test$rating, mu)`). The calculated RMSE is shown in Table 8.

```{r naive_rmse}
mu <- mean(train$rating)
```

```{r naive_results, echo=FALSE}
rmse_results <- tibble(Method = "Just the Average", RMSE = RMSE(test$rating, mu))
kbl(rmse_results, booktabs = TRUE, caption = 'RMSE result for the Average Method') %>%  kable_styling(latex_options = c("striped","HOLD_position"))
```

We can observe that this value is actually the standard deviation of this distribution. Its value is too high, thus not acceptable.

$$
\mbox{RMSE}_{\mu} = \sqrt{\frac{1}{N}\sum_{u,i}^{N}(\mu-y_{u,i})^2} = \sigma
$$

### Movie effect model

We can improve our model by adding a term, $b_i$, that represents the effect of each movie's average rating:

$$
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
$$

Here, $b_i$ is approximately the average of $Y_{u,i}$ minus the overall mean for each movie $i$. We can again use least squares to estimate $b_i$ as follows:

```{r lm_function, eval=FALSE}
fit <- lm(rating ~ as.factor(movieId), data=edx)
```

Because there are thousands of b's, the `lm()` function will be very slow or cause R to crash, so it is not recommendable using linear regression to calculate these effects [@rafalab]. We will then calculate all $b_i$'s in the way shown below, followed by the determination of which $b_i$ corresponds to each line in the test set and addition of $\mu$ to them, obtaining our estimates for each rating.

```{r movie_effects}
movie_avgs <- train %>% group_by(movieId) %>% summarize(b_i = mean(rating-mu))

predicted_ratings <- mu + test %>% left_join(movie_avgs, by='movieId') %>%
  pull(b_i)

summary(predicted_ratings)
```

From the summary above, we can see that the predictions obtained range from 0.5 and 4.75, which are values inside the real range (0.5 to 5). Calculating `RMSE(test$rating, predicted_ratings)`, we get the results shown in Table 9. 

```{r movie_effects_results, echo=FALSE}
rmse_results <- bind_rows(rmse_results,
                          tibble(Method="Movie effect model",  
                                 RMSE = RMSE(test$rating, predicted_ratings)))
kbl(rmse_results, booktabs = TRUE, caption = 'RMSE results with the Movie effect model') %>%  kable_styling(latex_options = c("striped","HOLD_position"))
```

A 12% drop is a big improvement, but we can still do better. 

### Movie + User effects model

We further improve our model by adding $b_u$, the user-specific effect:

$$ 
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
$$

To fit this model, we could again calculate the least squares estimate by using `lm` [@rafalab]:

```{r lm_movie_user, eval=FALSE}
lm(rating ~ as.factor(movieId) + as.factor(userId))

```

But for the same reason as explained before, we will follow a simpler model approach where $b_u$ is calculated with the difference between rating $Y_{u,i}$ and both overall mean rating and movie-specific coefficient. It is demonstrated in the code chunk below: 

```{r m_u_effects}
user_avgs <- train %>%  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%  summarize(b_u = mean(rating - mu - b_i))

predicted_ratings <- test %>%
  left_join(movie_avgs, by='movieId') %>%  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%  pull(pred)

summary(predicted_ratings)
```

Predicted ratings, obtained from the sum of $\mu$ to each test set line's corresponding coefficients, now have a spread of -0.7161 to  6.0547, which is bigger than the real spread. There are `r sum(predicted_ratings<0.5)` values under 0.5 and `r sum(predicted_ratings>5)` over 5. But what we really need to evaluate is the RMSE value by applying its function.

```{r m_u_effects_results, echo=FALSE}
rmse_results <- bind_rows(rmse_results,
                          tibble(Method="Movie + User effects model",  
                                 RMSE = RMSE(test$rating, predicted_ratings)))
kbl(rmse_results, booktabs = TRUE, caption = 'RMSE results with the Movie and User effects model') %>%  kable_styling(latex_options = c("striped","HOLD_position"))

rm(predicted_ratings)
```

Table 10 shows another big improvement in the RMSE value, with a 8% drop. Let's proceed improving this model to see how much better it can get.

### Movie + User + Genre effects model

Now we try an approach proposed by @rafalab, which was presented but not built up and developed, that adds the genre effect to the model. Its formula is written as shown below:

$$
Y_{u,i} = \mu + b_i + b_u + \sum_{k=1}^K x^k_{u,i} \beta_k + \varepsilon_{u,i}
$$

Here $x^k_{u,i}$ is equal to $1$ if $g_{u,i}$ is genre $k$. We already determined $x_{u,i}$ values earlier by creating binary columns for each of the 20 genres available, both in the training and test data. Now we determine $\beta_k$ values with the same approach utilized in the last two models, where each $\beta_k$ is the mean of the subtraction of overall average rating, movie-specific coefficient $b_i$ and user-specific coefficient $b_u$ from ratings $Y_{u,i}$. This way, if a movie belongs to both $Comedy$ and $Drama$, one $\beta$ will throw the average rating up (high ratings for $Drama$) and the other down (low average for $Comedy$).

To make all process faster down the line, we will first add $b_i$ and  $b_u$ columns to the training and test sets, and then we calculate all $\beta_k$'s.

```{r beta_genre}
train <- train %>% left_join(movie_avgs, by='movieId') %>% 
  left_join(user_avgs, by='userId') 
test <- test %>% left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId')
rm(user_avgs,movie_avgs)

beta_k <- vector()
for (i in seq_along(all_genres)) {
  b_value<- train %>% 
    group_by(!!sym(all_genres[[i]])) %>%          
    summarize(beta_k=mean(rating-mu-b_i-b_u)) %>% 
    filter((.[1])==1) %>% .[[2]]                  
  beta_k <- append(beta_k, b_value)               
}
rm(i,b_value)

df_beta<-data.frame(beta_k) 
rownames(df_beta) <- all_genres
```

To determine these values within a `for()` loop, we first grouped the training set per specific genre column, with the aid of argument `!!` and function `sym` to unquote column names, forming two different groups (genre = 0 and genre = 1). We then obtained each specific genre's beta-value by filtering for values equal to 1 (the genre is present). We appended each one to the `beta_k` vector and when all $\beta$'s have been obtained, we transformed this vector into the dataframe shown in Table 11.

```{r df_beta_divided, echo=FALSE}
df_beta_div <-data.frame(row.names(df_beta)[1:10], df_beta[1:10,],
                         row.names(df_beta)[11:20], df_beta[11:20,]) # Dividing table in two for better placement in report
colnames(df_beta_div) <- c('genre','beta_k','genre','beta_k')
df_beta_div %>% kbl(booktabs = TRUE, linesep = "", caption = 'Genre-specific beta values') %>% column_spec(1:4, width='8em') %>% kable_styling(latex_options = c("striped","HOLD_position")) %>% add_header_above(c("df_beta[1:10]" = 2, "df_beta[11:20]" = 2))
```

The summation within this model's prediction equation is represented below:

$$
\sum_{k=1}^K x_{u,i}\beta_{k} = x^{\{1\}}_{u,i}\beta_{1} +  x^{\{2\}}_{u,i}\beta_{2} + ... +  x^{\{20\}}_{u,i}\beta_{20}
$$

We calculate these sums by performing a matrix multiplication between the test set genre columns (`test[,4:23]`) and all 20 $\beta_k$ values calculated. This multiplication, between a (`r dim(test)[1]`, 20) matrix and a (20, 1) vector, results in a (`r dim(test)[1]`, 1) vector. We then add this as a column to the test set, so we can calculate predictions.

```{r m_u_g_effects, warning=FALSE}
sum_x_beta <- as.matrix(test[,4:23])%*%as.matrix(df_beta) 
sum_x_beta <- data.frame(sum_x_beta=sum_x_beta[,1])       

test <- data.frame(test, sum_x_beta)
rm(df_beta,beta_k,sum_x_beta,all_genres)

predicted_ratings <- test %>%
  mutate(pred = mu + b_i + b_u + sum_x_beta) %>%
  pull(pred)

summary(predicted_ratings)
```

Analyzing the values obtained for ratings, we see there are `r sum(predicted_ratings<0.5)` values under 0.5 and `r sum(predicted_ratings>5)` predicted ratings over 5.

```{r m_u_g_effects_results, echo=FALSE}
rmse_results <- bind_rows(rmse_results,
                          tibble(Method="Movie + User + Genre effects model",  
                                 RMSE = RMSE(test$rating, predicted_ratings)))
kbl(rmse_results, booktabs = TRUE, caption = 'RMSE results with the Movie, User and Genre effects model') %>%  kable_styling(latex_options = c("striped","HOLD_position"))
rm(predicted_ratings)
```

While the work needed to obtain this prediction model was arduous, there was very little improvement from the previous model (as seen in Table 12), so we will try adding regularization.

### Regularized Movie + User effects model

When a movie receives ratings from just a few users, or users give very few reviews overall, we have more uncertainty. Therefore, larger estimates of both $b_i$ and $b_u$, negative or positive, are more likely [@rafalab]. Consider a case in which we have a movie with 500 user ratings and another film with just one. While the first movie's $b_i$ can be pretty accurate, the estimate for the second movie will simply be the observed deviation from the average rating, which is a clear sign of overtraining. In this cases, it is better to predict it as just the overall average rating $\mu$, which is why we need a form of model penalization. Regularization permits us to penalize large estimates that are formed using small sample sizes.

We will add regularization to one of the previously tried models. Since adding genre didn't have much effect on RMSE, and both optimal $\lambda$ determination (which we'll perform in this section) and $\beta_k$ calculations are computationally costly, we will use the Movie + User effects model. So now, instead of minimizing the least squares equation, we minimize an equation that adds a penalty. We need to apply regularization to both user and movie effects, and we do it as follows:

$$
\sum_{u,i} \left(y_{u,i} - \mu - b_i - b_u \right)^2 + 
\lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2\right)
$$

The formula below shows the new form of calculating $b_i$, with the inclusion of $\lambda$ in the mean values calculation. When our sample size $n_i$ is very large, then the regularization cost $\lambda$ is effectively ignored since $n_i+\lambda\approx n_i$ [@rafalab]. However, when $n_i$ is small, then the estimate  $\hat{b}_i(\lambda)$ is shrunken towards 0. The larger $\lambda$, the smaller $b_i$.

$$
\hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{i=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)
$$

In the same way, $\hat{b}_u(\lambda)$ is the regularized $b_u$, which includes $\lambda$ in the division denominator.

$$
\hat{b}_u(\lambda) = \frac{1}{\lambda + n_u} \sum_{u=1}^{n_u} \left(Y_{u,i} - \hat{b}_{i} - \hat{\mu}\right)
$$

We first calculate various sets of predictions and their respective RMSE values by testing $\lambda$'s from 0 to 10, with a 0.25 step-size, and then determine which regularization parameter gives us the lowest RMSE.

```{r regularized_m_u_effects, fig.height=3, fig.fullwidth=TRUE , fig.align='center', fig.cap='RMSE values per lambda'}
train <- train %>% select(userId, movieId, rating)
test <- test %>% select(userId, movieId, rating)

lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  b_i <- train %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))       
  b_u <- train %>%
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l)) 
  predicted_ratings <-
    test %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)                                      
  return(RMSE(test$rating, predicted_ratings))
})
qplot(lambdas, rmses, xlab='lambda', ylab='RMSE')
```

By analyzing the plot and retrieving values from table `rmses`, we identify that the optimal $\lambda$ value is `r lambdas[which.min(rmses)]`, and its corresponding RMSE is `r min(rmses)`.

```{r optimal_trained_model, echo=FALSE}
b_i <- train %>% group_by(movieId) %>%
summarize(b_i = sum(rating - mu)/(n()+5))

b_u <- train %>% left_join(b_i, by="movieId") %>% group_by(userId) %>%
summarize(b_u = sum(rating - b_i - mu)/(n()+5)) 

predicted_ratings <- test %>%
left_join(b_i, by = "movieId") %>%
left_join(b_u, by = "userId") %>%
mutate(pred = mu + b_i + b_u) %>%
pull(pred)                                      

summary(predicted_ratings)
```

As usual, the `summary` function shows us that there are predicted values under 0.5 (`r sum(predicted_ratings<0.5)` predictions) and  over 5.0 (`r sum(predicted_ratings>5)`).

```{r lambda_rmse_results, echo=FALSE}
rmse_results <- bind_rows(
  rmse_results,
  tibble(Method="Regularized Movie + User effects model",  
         RMSE = min(rmses)))
kbl(rmse_results, booktabs = TRUE, caption = 'RMSE results with the Regularized Movie and User effects model') %>%
  kable_styling(latex_options = c("striped","HOLD_position"))

rm(lambdas,rmses,mu) # Cleaning variables to recover RAM space
```

Once more there is improvement, but not much. The approach we are taking seems to begin stagnating, so we try a different one: matrix factorization.

### Matrix Factorization with _recosystem_ R package

Some groups of movies, as well as some groups of users, have similar rating patterns. We can identify them by inspecting their correlations, which is basically what matrix factorization does. This family of methods, which is a class of collaborative filtering algorithms used in recommendation systems that became widely known during the _Netflix_ challenge, works by decomposing the (incomplete) user-movie interaction matrix into the product of two lower dimensionality rectangular matrices [@mf-rec-sys]. This incomplete matrix $R_{m×n}$, as mentioned in Section 2.2.2, is composed by users on the rows and movies on the columns, with each cell being an attributed rating (or the absence of one).

So we approximate the whole rating matrix $R_{m×n}$ by the product of two matrices of lower dimensions, $P_{k×m}$ and $Q_{k×n}$, as described above and written below [@recosys].

$$
R \approx P'Q
$$

Let $p_u$ be the $u$-th column of $P$, and $q_i$ be the $i$-th column of $Q$, then the rating given by user $u$ on item $i$ would be predicted as $p_u'q_i$. A typical solution for P and Q is the following [@sgd-mf; @lr-sgd]:


$$
\min_{P,Q} \sum_{(u,i)∈R}[f(p_u,q_i;r_{u,i})+\mu_P||p_u||_1+\mu_Q||q_i||_1+\frac{\lambda_P}{2}||pu||^2_2+\frac{λ_Q}{2}||q_i||_2^2]
$$

where $(u,i)$ are locations of observed entries in $R$, $r_{u,i}$ is the observed rating, $f$ is the loss function, and $μ_P$, $μ_Q$, $λ_P$ and $λ_Q$ are, respectively, pairs of L1 and L2 regularization costs for both P and Q during gradient descent, to avoid overtraining.

The _recosystem_ package easily performs this factorization. But to do it, we need to input a parse matrix in triplet form, meaning that each line in the file must contain three numbers: `user_index`, `item_index` and `rating` [@recosys]. User and item indexes may start with either 0 or 1, and this can be specified by the `index1` parameter in `data_memory()`. To train and predict, we need to provide objects of class `DataSource`, such as the previously mentioned `data_memory()`, which saves datasets as R objects.

After determining the training and test sets, we define an R object with function `Reco()` and tune it to identify the best parameters possible for our dataset. We need to determine the number of latent factors ($k$ rows for $P_{k×m}$ and $Q_{k×n}$)  with parameter `dim`, L2 regularization costs with `costp_l2` and `costq_l2`, and learning rate for gradient descent `lrate`. The `loss` parameter has a default value of `l2`, so we maintain it and keep L1 costs as zero.

```{r recosystem_training, warning=FALSE, message=FALSE, results='hide'}
train_dm <- data_memory(user_index = train$userId, item_index = train$movieId,
                          rating = train$rating, index1 = TRUE) 
rm(train)

test_dm <- data_memory(user_index = test$userId, item_index = test$movieId, 
                       index1 = TRUE)                           
test_rating <- test$rating                                      
rm(test)

set.seed(123, sample.kind = "Rounding")
r <- Reco()                            
params = r$tune(train_dm, opts = list(dim = c(15, 20),
                                        costp_l1 = 0,
                                        costp_l2 = c(0.01, 0.1),
                                        costq_l1 = 0,
                                        costq_l2 = c(0.01, 0.1),
                                        lrate = c(0.075, 0.1), nthread = 2))

optimal_params = params$min

r$train(train_dm, opts = c(optimal_params, nthread = 1, niter = 20))
```

 From tuning, we obtained optimal parameters `dim` = `r optimal_params[[1]]`, `costp_l1` = `r optimal_params[[2]]`, `costp_l2` = `r optimal_params[[3]]`, `costq_l1` = `r optimal_params[[4]]`, `costql2`=`r optimal_params[[5]]` and `lrate` = `r optimal_params[[6]]`. The model was then trained with these values, and it is now possible to predict ratings for the test set with `r$predict()`.

```{r predicting_MF, message=FALSE}
predicted_ratings = r$predict(test_dm, out_memory()) 

summary(predicted_ratings)
```

Our predicted ratings are obtained with an object of class `Output`, called `out_memory()`, and is returned as an R object. Inspecting the predictions obtained, we observe there are `r sum(predicted_ratings<0.5)` values under 0.5 and `r sum(predicted_ratings>5)` over 5 stars rating. Table 14 shows the results for `RMSE(test_rating, predicted_ratings)`.

```{r MF_results, echo=FALSE}
rmse_results <- bind_rows(rmse_results,
                          tibble(Method="Matrix Factorization Model",  
                                 RMSE = RMSE(test_rating, predicted_ratings)))

kbl(rmse_results, booktabs = TRUE, linesep = "", caption = 'RMSE results with the Matrix Factorization Model') %>%  
  kable_styling(latex_options = c("striped","HOLD_position"))

rm(test_rating,predicted_ratings,params,test_dm)
```

This model provides us a much better result, so we choose it to perform the final evaluation on the final hold-out set, `validation`, and finally conclude this report.

# Results

## Training final model with full edx dataset

To guarantee this final prediction model can be applied to the whole validation set, we will perform training with a full `edx` set, in the same way done in the previous Section. Once again, training will be performed with the optimal parameters defined by the tuning process.

```{r recosystem_edx, message=FALSE, results='hide'}
train_dm <- data_memory(user_index = edx$userId, item_index = edx$movieId, 
                          rating = edx$rating, index1 = TRUE)
rm(edx)

r$train(train_dm, opts = c(optimal_params, nthread = 1, niter = 20))
```

## Final predicted ratings and RMSE

We finally predict ratings for the validation dataset, and get the following range of values:

```{r prediction_edx, message=FALSE}
validation_dm <- data_memory(user_index = validation$userId,
                             item_index = validation$movieId, index1 = TRUE)
validation_rating <- validation$rating

predicted_ratings = r$predict(validation_dm, out_memory()) 
rm(train_dm,validation)

summary(predicted_ratings)
```

We perceive there are `r sum(predicted_ratings<0.5)` values under 0.5 rating and `r sum(predicted_ratings>5)` over 5 stars. But the real model evaluation happens with RMSE.

```{r results_validation, echo=FALSE}
rmse_validation <- tibble(Method="Matrix Factorization Model",  
                                 RMSE = RMSE(validation_rating, predicted_ratings))
colnames(rmse_validation) <- c('Method','Validation RMSE')
kbl(rmse_validation, booktabs = TRUE, caption = 'RMSE result for the validation set with Matrix Factorization Model') %>%  kable_styling(latex_options = c("striped","HOLD_position"))
```

Table 15 provides us the final RMSE, the Validation one, which reaches the value of `r RMSE(validation_rating, predicted_ratings)`. It is an excellent result, if compared to other models in this report and also to the winning RMSE value in the _Netflix_ challenge, which was 0.8712 [@bellkor]. Even though that challenge utilized a different dataset, it is still remarkable. 

# Conclusion

We began this report by obtaining the data and generating our dataframes, followed by visualizations of ratings distribution and rating variability between different movies and users. Genre and genre combinations also showed strong effects on ratings, while some attributes did not add significant value, such as the time of rating effect. 

A lot of models were evaluated, starting with simpler ones like the Overall Average Rating and Movie effects models. More complex models were evaluated, like Movie + User effects and specially Movie + User + Genre effects, but since it didn't have a great influence on the RMSE value, we didn't utilize it during the following regularized modeling. The penultimate and regularized model didn't improve much either. The best and easiest model applied was Matrix Factorization with _recosystem_ R package. From the third model, we had already reached the Netflix prize-winning RMSE, and by the end a value under 0.8 was reached.  

As future improvement, we can determine ceiling and roof limits for the predictions. That is: we transform predicted values under 0.5 stars or over 5 into the limit values from the true range of 0.5 to 5, further improving our RMSE results. We can also tune the Matrix Factorization model with a wider range of values to further optimize it.

