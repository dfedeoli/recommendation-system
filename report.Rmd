---
title: "Rating Prediction with MovieLens 10M Dataset"
author: "Danilo Ferreira de Oliveira"
date: "04/12/2021"
output: 
  pdf_document:
    highlight: tango
    latex_engine: xelatex
    number_sections: true
    toc: true
    toc_depth: 3
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) #fig.pos = 'h' fig.align = 'center' fig.cap
knitr::opts_chunk$set(comment = '>>' )
knitr::opts_chunk$set(tidy = TRUE)
```

# Introduction

For this project, we will be creating a movie recommendation system using the MovieLens dataset. You can find the [entire latest MovieLens dataset here](https://grouplens.org/datasets/movielens/latest/). You will be creating your own recommendation system using all the tools shown throughout the courses in the HarvardX: Data Science Professional Certificate series. We will use the [10M version of the MovieLens dataset](https://grouplens.org/datasets/movielens/10m/) to make the computation a little easier.

You will train a machine learning algorithm using the inputs in one subset to predict movie ratings in the validation set. 

Recommendation systems are more complicated machine learning challenges because each outcome has a different set of predictors. For example, different users rate a different number of movies and rate different movies. By evaluating the movies specific users like to watch, we can predict ratings they would give for movies they haven't seen, and then recommend them those with relatively high ratings.

We will firstly follow the approach made by Professor Rafael Irrizarry in [Introduction to Data Science](https://rafalab.github.io/dsbook), and continue it with the addition of genre effects, and a model that uses matrix factorization.

To compare different models or to see how well we are doing compared to a baseline, we will use root mean squared error (RMSE) as our loss function. We can interpret RMSE similar to standard deviation.

ou will use the following code to generate your datasets. Develop your algorithm using the edx set. For a final test of your final algorithm, predict movie ratings in the validation set (the final hold-out test set) as if they were unknown. RMSE will be used to evaluate how close your predictions are to the true values in the validation set (the final hold-out test set).

# Analysis

## Getting the data

```{r install_packages, echo=FALSE, warning=FALSE,message=FALSE}
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(rlist)) install.packages("rlist", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(recosystem)) install.packages("recosystem", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(tibble)) install.packages("tibble", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(cowplot)) install.packages("cowplot", repos = "http://cran.us.r-project.org")
```

 Before we can start gathering data, we will first load the R libraries we will need along all the report.

```{r libraries, warning=FALSE,message=FALSE}
library(caret)     
library(tidyverse) 
library(ggplot2)
library(lubridate)
library(stringr)
library(recosystem)
library(data.table)
library(kableExtra)
library(tibble)
```

Then, we download the MovieLens data and run code provided by HarvardX to generate the datasets for training and evaluating the final model. The `edx` set will be used all along the report to train and choose the best model for rating predictions, while the `validation` set will only be used at the end of to evaluate the quality of the final model with the root mean square error. 

```{r getting_data}
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")
```

The validation set will be 10% of the MovieLens data obtained. We need to make sure the user and movie IDs in `validation` are also in `edx`, so we apply the `semi_join()` transformations. Then, we add rows removed from the validation set back into the training set. Lastly, we clean variables to recover RAM space.

```{r creating_edx_validation, warning=FALSE, message=FALSE, results='hide'}
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

We can now start analyzing the dataframe `edx`.

## Exploratory data analysis and visualization

### Exploring the variables

Let's take a look at the columns in our dataset. 

```{r head_of_edx}
edx %>% as_tibble() 
```

It is possible to see that columns `title` and `genres` are of character class (<chr>), while `movieId` and `rating` are numeric (<dbl>), `userId` and `timestamp` are integers (<int>).

We should analyze the quantity of unique values for each column variable. 

```{r summary_n_distinct}
edx %>% summarize(n_userIds = n_distinct(userId), n_movieIds = n_distinct(movieId),
                  n_titles = n_distinct(title), n_genres = n_distinct(genres)) %>% 
  kbl(booktabs = TRUE,
      caption = "Distinct values for userId, movieId, titles and genres") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

Note that there more `movieId` unique variables than there are `title` ones. We need to investigate why.

```{r movies_with_multiple_Ids}
edx %>% group_by(title) %>% 
  summarize(n_movieIds = n_distinct(movieId)) %>% filter(n_movieIds > 1) %>% 
  kbl(booktabs = TRUE, caption = "Movies with more than one ID") %>% 
  kable_styling(latex_options = c("striped","hold_position"))
```

So we find out that _War of the Worlds (2005)_ has two distinct `movieIds`. Let's see how many reviews for each of them there are.

```{r movieIds_for_WW2005, echo=FALSE}
edx %>% filter(title=='War of the Worlds (2005)') %>% 
  group_by(movieId) %>% summarize(title=title[1], genres=genres[1], n=n()) %>%  # Count of reviews for each movieId
  kbl(booktabs = TRUE, caption = "Different values of movieId and genres for \\textit{War of the Worlds (2005)}") %>% kable_styling(latex_options = c("striped","hold_position"))
```

The second `movieId` received only 28 reviews. It is a low value and the validation set possibly has these two movieIds for the movie, so we won't meddle with it.

### Ratings

We can now visualize the data. Let's see how the ratings distribution is configured.

```{r ratings_distribution, echo=FALSE, fig.height=3, fig.fullwidth=TRUE, fig.align='center', fig.cap = "Distribution of ratings"}
edx %>% mutate(stars=ifelse(rating %in% c(1:5),'full','half')) %>% 
  ggplot(aes(factor(rating),fill=stars)) + geom_bar() + scale_fill_hue(c=80) + 
  labs(x='rating')
```

From the plot above, we can affirm that full star ratings are more common than those with half stars, since 4, 3 and 5 are the most frequent ones.

### Movie and user IDs

```{r hist_count, echo=FALSE, fig.align='center', fig.height=3, fig.cap= 'Frequency of reviews given by a specific user (left) and of reviews received by a specific movie (right)'}
p1 <- edx %>% count(userId) %>% ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + scale_x_log10() + 
  labs(x='reviews given by user') # Number of reviews from each user

p2 <- edx %>% count(movieId) %>% ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + scale_x_log10() +
  labs(x='reviews received by movie',y='') # Number of reviews of each movie

gridExtra::grid.arrange(p1, p2, ncol = 2)
rm(p1,p2)
```

The plots above show the frequency of quantities of reviews that each user gives and quantities of reviews each movie receives. Their x-axis are both on a logarithmic scale, and the one on the left is *rightly skewed*. If we analyze them, we can say that the majority of the users give from 20 to 200 reviews, while each movie frequently receive between 20 to a 1000 reviews, which is a pretty broad range.

Plotting the histogram for average ratings, we get the plots below for both users and movies. It is possible to see that it is rare for a user to average rating movies below 2, since the top plot shows there are few users to the left of that number, only `r edx %>% group_by(userId) %>% summarize(avg=mean(rating)) %>% filter(avg< 2) %>% dim() %>% .[1]` of all `r edx %>% summarize(n_userIds = n_distinct(userId)) %>% .[1,1]` in the dataset, to be more exact. 

```{r hist_avg_rating, echo=FALSE, fig.align='center', fig.height=3, fig.cap='Frequency of average rating a user gives (top) and average rating a movie receives (bottom)'}
p1 <- edx %>% group_by(userId) %>% summarize(avg=mean(rating)) %>% 
  ggplot(aes(avg)) + geom_histogram(bins=35, color='black') +
  labs(x='average rating given by user') #average rating a user gives

p2 <- edx %>% group_by(movieId) %>% summarize(avg=mean(rating)) %>% 
  ggplot(aes(avg)) + geom_histogram(bins=35, color='black') +
  labs(x='average rating received by movie') # Average rating received by a movie

cowplot::plot_grid(p1,p2,ncol=1, align='v')
rm(p1,p2)
```

From all 10,676 movies, we can see that only `r edx %>% group_by(movieId) %>% summarize(avg=mean(rating)) %>% filter(avg< 1.5) %>% dim() %>% .[1]` of them average below 1.5, and `r edx %>% group_by(movieId) %>% summarize(avg=mean(rating)) %>% filter(avg> 4.25) %>% dim() %>% .[1]` films average over 4.25 rating.

Let's now explore the top 10 most reviewed movies in the dataset and how many times they were reviewed.

```{r unique_movies, echo=FALSE, fig.height=4, fig.fullwidth=TRUE, fig.align='center', fig.cap='Number of reviews for the top 10 most reviewed movies'}
unique_movies <- edx %>% group_by(title) %>% 
  summarize(title=title[1], genres=genres[1], count=n()) %>% arrange(desc(count))
unique_movies[1:10,] %>% mutate(title = str_remove(title, ", The"),
                                title = str_remove(title, " - A New Hope \\(a.k.a. Star Wars\\)")) %>% 
  ggplot(aes(x=reorder(title, count), y=count)) + geom_bar(stat = "identity") + labs(x='') + coord_flip()
```

As we can see, the most reviewed movies are normally blockbusters, and they also have really high ratings. We can infer that the most times a movie is reviewed, the better its rating is. 

```{r movies_ratings}
edx %>% filter(title %in% head(unique_movies$title,n=10)) %>% group_by(title) %>% 
  summarize(avg=mean(rating)) %>% arrange(desc(avg))
```


the title of the movies come along with the release date, inside parenthesis.

### Release date

Since there isn't a column for each movie's release date, We will check it by using the `str_extract` function from the `stringr` package.

```{r release_date}
release_date <- edx$title %>% str_extract('\\([0-9]{4}\\)') %>%
  str_extract('[0-9]{4}') %>% as.integer()
summary(release_date)
```

We first apply this function extracting the date along with the parenthesis, and then the number from inside them. We do it like this because by extracting only the number, movies like _2001: A Space Odyssey (1968)_ and _2001 Maniacs (2005)_ show up when searching for year 2001, for example. To go even further, it also extracts the numbers 1000 and 9000 for _House of 1000 Corpses (2003)_ and _Detroit 9000 (1973)_, respectively.

Below we can see that the earliest release date in our dataset (1915) only has one movie:

```{r released_edx, echo=FALSE}
edx <- data.frame(edx,released=release_date)
edx[which(edx$released==1915),] %>% summarize(movieId=movieId[1], title=title[1], genres=genres[1], n=n()) %>% kbl(booktabs = TRUE, linesep = "", caption = "Movie with the earliest release date in the dataset") %>% kable_styling(latex_options = c("striped","hold_position"))
```

We now compute the number of ratings for each movie and then plot it against the year the movie came out, using a boxplot for each year. The logarithmic transformation is applied on the y-axis (number of ratings) when creating the plot.

```{r released_count, echo=FALSE, fig.fullwidth=TRUE, fig.align='center', fig.height=4, fig.cap="Number of reviews the movies received versus the year they were released"}
edx %>% group_by(movieId) %>%
  summarize(n = n(), released = as.character(first(released))) %>%
  ggplot(aes(released, n)) + geom_boxplot(fill= 'orange', color='#808080', alpha=0.2) +
  scale_y_log10(breaks=c(1,10,100,200,500,1000,10000)) + scale_x_discrete(breaks=seq(1915,2005,5)) + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) + labs(x='year released',y='count')
```

We see that, on average, movies that came out in the mid 90's get more ratings, reaching values over 30,000 reviews. We also see that with newer movies, starting near 1998, the number of ratings decreases with year: the more recent a movie is, the less time users have had to rate it.

Now we compute the average of ratings for each movie and plot it against the release year with boxplots, similarly to before.

```{r released_avg_rating, echo=FALSE, fig.fullwidth=TRUE, fig.align='center', fig.height=4, fig.cap="Average rating the movies received versus the year they were released"}
edx %>% group_by(movieId) %>%
  summarize(n = n(), released = as.character(first(released)), avg_rating=mean(rating)) %>%
  ggplot(aes(released, avg_rating)) + geom_boxplot(fill= 'orange', color='#808080', alpha=0.2) + scale_x_discrete(breaks=seq(1915,2005,5)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) + labs(x='year released', y='average rating')
edx <- edx[,-ncol(edx),drop=FALSE]
rm(release_date)
```

It is possible to see that most movies released until 1973 normally range between 3.0 and 4.0 ratings. From then on, the range gets bigger and values get lower. The median value goes from averaging over 3.5 to near 3.25. There are even some movies from around the 2000's that average 0.5 ratings, but they are outliers and probably only received very few ratings.

### Genres and genre combinations

The `genres` variable actually represents combinations of a series of unique genres. We will count the number of reviews for all different combinations that appear in our dataset and arrange them in descending order. The table below shows, as an example, the top 7 most reviewed genre combinations.

```{r genre_combinations, echo=FALSE}
edx %>% group_by(genres) %>% summarize(count = n()) %>% arrange(desc(count)) %>% head(n=7) %>% 
  data.frame(rank=c(1:7), .) %>% kbl(booktabs = TRUE, linesep = "", caption = 'Top genre combinations rated') %>% kable_styling(latex_options = c("striped","hold_position"))
```

Let's check the genre effect on ratings. We will filter for genre combinations that were reviewed over a thousand times, put them in ascending order of average rating and show 15 of them, equally distant inside the arranged dataframe. The indexes to select the genres in the arranged set are represented by the variable `gcomb_15`.

```{r genre_effect, fig.fullwidth=TRUE, fig.align='center', fig.height=4, fig.cap="Distribution of average ratings for a few genre combinations possible"}
gcomb_15 <- c(1,seq(31,415,32),444) 
edx %>% group_by(genres) %>%
  summarize(n = n(), avg = mean(rating), se = sd(rating)/sqrt(n())) %>%     
  filter(n > 1000) %>% arrange(desc(avg)) %>%  .[gcomb_15,] %>%
  mutate(genres = reorder(genres, avg)) %>%
  ggplot(aes(x = genres, y = avg,
             ymin = avg - 2*se, ymax = avg + 2*se)) + geom_point() + 
  geom_errorbar() + theme(axis.text.x=element_text(angle = 30, hjust = 1)) + 
  labs(x='', y= 'average rating')
```

From the plot above, it is clear the effect genre combinations have on the rating, since they range from approximately 2.0 to over 4.5. Let's now see the effect of each genre separately.

There are 20 different genres in this dataset, and they are the following:

```{r all_genres, echo=FALSE}
rm(gcomb_15)
genre_combinations <- unique(edx$genres)     # Unique genre combinations
all_genres <- character()                    # Empty character vector
for (i in genre_combinations) {
  movie_genres <- str_split(i,'\\|')[[1]]    # Splitting the genre combinations
  for (j in movie_genres) {
    all_genres <- rlist::list.append(all_genres, j) # Appending each genre
  }
}
all_genres <- unique(all_genres)             # Unique genres in the dataset
rm(genre_combinations,movie_genres,i,j)
matrix(all_genres, nrow = 5, ncol= 4) %>% kbl(booktabs = TRUE, linesep = "", caption = 'All unique genres present in the dataset') %>% column_spec(1:4, width='8em') %>% kable_styling(latex_options =c("striped","hold_position"))
```

We can count how many reviews each genre have had, by applying the following function:

```{r all_genres_count}
all_genres_count <- sapply(all_genres, function(g) {
  sum(str_detect(edx$genres, g))      
}) %>% data.frame(review_count=.) %>% 
  arrange(desc(.))

```

We can see the results on the table below. The genres range from near 4 million to only 7 reviews.    

```{r all_genres_count_div, echo=FALSE}
all_genres_count_div <-data.frame(row.names(all_genres_count)[1:10], all_genres_count[1:10,],
                                  row.names(all_genres_count)[11:20], all_genres_count[11:20,]) # Dividing table in two for better placement in report
colnames(all_genres_count_div) <- c('Genre','Count','Genre','Count')
all_genres_count_div %>% kbl(booktabs = TRUE, linesep = "", caption = 'Number of reviews per genre') %>% column_spec(1:4, width='9em') %>% kable_styling(latex_options =c("striped","hold_position")) %>% add_header_above(c("all_genres_count[1:10]" = 2, "all_genres_count[11:20]" = 2)) # all_genres_count[1:10]
 
```

We need to investigate the `(no genres listed)` genre, which actually represents no genre at all. We can see the movies that belong to it and their respective averages and standard errors:

```{r no_genres_listed}
edx[which(edx$genres=='(no genres listed)'),] %>% 
  summarize(movieId=movieId[1], userId=userId[1], title=title[1], genres=genres[1],
            n = n(), avg = mean(rating), se = sd(rating)/sqrt(n()))
```

There is only one movie in the dataset with no genres listed, and it has only 7 reviews, giving its average rating a high standard error (0.4185). For this reason, we won't be plotting this genre's average rating along with the others.

The plot below shows the average ratings for each unique genre, as they range from under 3.3 to over 4.0.

```{r all_genres_avg_rating, echo=FALSE, fig.align='center', fig.width=6, fig.length=3.5, fig.cap='Average rating for each genre'}
all_genres_avg <- matrix(nrow=20,ncol=2)
i <- 1
for (var in all_genres) {
  vals <- edx[which(str_detect(edx$genres,as.character(var))),] %>% 
    summarize(n = n(), avg = mean(rating), se = sd(rating)/sqrt(n()))
  all_genres_avg[i,1] <- vals$avg
  all_genres_avg[i,2] <- vals$se
  i <- i+1
}
data.frame(genres=all_genres, avg=all_genres_avg[,1], se=all_genres_avg[,2]) %>% .[1:19,] %>%
  mutate(genres = reorder(genres, avg)) %>%
  ggplot(aes(x = genres, y = avg, ymin = avg - 2*se, ymax = avg + 2*se)) +  # Plotting the values and errors
  geom_point() + geom_errorbar() + theme(axis.text.x=element_text(angle = 45, hjust = 1)) + 
  labs(x='', y= 'average rating')
rm(var,i,vals,all_genres_avg)
```


We now observe the top genres reviewed in this dataset, and how many reviews each one of them have. We accounted for the genres that were reviewed over a million times, resulting in 8 distinct possibilities. The reviews count for all genre has already been shown, but we will plot the count for the top 8 genres to better visualize it, and also compare its shape with the movie count plot for the top 8. 

```{r top_genres_reviews, echo=FALSE, fig.width=6, fig.height=3, fig.fullwidth=TRUE, fig.align='center', fig.cap='Number of reviews for each of the top 8 most reviewed genres'}
top_genres <- all_genres_count %>% filter(.>=1e6) # Genres with over 1 million reviews

top_genres %>% ggplot(aes(x= row.names(.), y = review_count, fill=row.names(.))) +
  geom_bar(stat='identity') + scale_fill_hue(c=40) + theme(legend.position="none") + 
  labs(x='', y='reviews count')
```

We now plot how many different movies where reviewed for each of the top 8 genres.

```{r movies_top_genre, echo=FALSE, fig.height=3, fig.fullwidth=TRUE, fig.align='center', fig.cap='Number of movies that belongs to the top 8 most reviewed genres'}
movies_per_top_genre <- sapply(row.names(top_genres), function(g) {
  sum(str_detect(unique_movies$genres, g))
})
data.frame(movies_per_top_genre) %>% 
  ggplot(aes(x= row.names(.), y = movies_per_top_genre, fill=row.names(.))) +
  geom_bar(stat='identity') + scale_fill_hue(c=40) + theme(legend.position="none") +
  labs(x='', y='movies count')
rm(movies_per_top_genre,top_genres,unique_movies,all_genres_count)
```

The general shape and proportion of the bars pretty much look the same for both reviews and movie counts.   

### Rating date

Since `timestamp` doesn't really have any value for us as it is, we create from it another column that gives us the date and time a movie was rated.

```{r timestamp_to_date}
edx <- mutate(edx, date = as_datetime(timestamp))
edx %>% as_tibble()
```

The new column `date` is of a Date-Time class (`<dttm>`) called POSIXct, and it clearly makes it better to understand the time of rating.

We can now plot the time effect on the dataset, showing the influence the week a movie was rated on the average value. Along with ratings, we plot a LOESS smooth line to better show the relationship between variables.

```{r time_effect, message=FALSE, fig.height=4, fig.fullwidth=TRUE, fig.align='center', fig.cap='The review date effect on the average rating'}
edx %>% mutate(date = round_date(date, unit = "week")) %>%
  group_by(date) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(date, rating)) +
  geom_point() + labs(y='average rating') +
  geom_smooth()
```

The review date definitely has some effect on ratings, but it isn't significant.

## Feature engineering

Down the line, we will need all genres available in the dataset as binary values, so it is necessary to create columns for all 20 of them and apply for each row the values correspoding for the character column value. here we see the first 6 rows, with only the movieId, title and genres of the movies, so we can compare the genres further in the analysis.

```{r binary_genres_pre_creation, tidy=TRUE}
edx[,c(2,5,6)] %>% as_tibble(.rows = 6)
```

We apply the transformation by doing the following `for()` command. We will also permanently remove the timestamp, title, genres and date columns, since we won't be needing them to train the prediction models. 

```{r binary_genres_columns, tidy=TRUE}
for (var in all_genres) {
  edx <- edx %>% mutate(genre=ifelse(str_detect(genres,as.character(var)),1,0))
  colnames(edx)[ncol(edx)] <- as.character(var)
}
rm(var)
edx <- edx[,-(4:7),drop=FALSE]
edx[,4:23] %>% as_tibble(.rows=6)
```

Comparing each row in this tibble above with the ones right before, it is possible to see that the genres in the first one are represented as 1s in the other, while genres that don't appear before are kept as zero values. The transformation occurred accurately.

## Creating training and test sets

Before training any model, we need to create a train and a test set. We first set the seed to guarantee the reproducibility of the model, and we divide the `edx` dataframe into a 80-20% partition.

```{r partitioning, warning=FALSE}
set.seed(123, sample.kind = "Rounding")
index <- createDataPartition(edx$rating, times=1, p=0.2, list= FALSE)
test <- edx[index,]
train <- edx[-index,]
```

```{r cleaning_index_edx, echo=FALSE}
rm(index)
edx <- edx %>% select(userId, movieId, rating)
```

To make sure the user and movie IDs in `test` are also in `train`, we apply the `semi_join()` function, just as we did in the beginning of the report for the first two datasets.

```{r test_semi_join}
test <- test %>% 
  semi_join(train, by = "movieId") %>%
  semi_join(train, by = "userId")
```

## Prediction models

Now that we have our training and test sets, we can start evaluating prediction models. We will begin by following the approach in Prof. Irrizarry's book, starting with just the Average Rating, going through the Movie Effect, Movie + User Effect and Regularized Movie + User Effects models. But we will also apply a method that includes the Genre Effect and one that perform Matrix Factorization. And to evaluate their quality, we will use the RMSE function.

 If N is the number of user-movie combinations, $y_{u,i}$ is the rating for movie i by user u, and $\hat{y}_{u,i}$ is our prediction, then RMSE is defined as follows: 

$$
\mbox{RMSE} = \sqrt{\frac{1}{N} \sum_{u,i}^{} \left( \hat{y}_{u,i} - y_{u,i} \right)^2 }
$$
We then write the function as follows:

```{r rmse_function}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

### Average Rating Value

We start with a model that assumes the same rating for all movies and all users, with all the differences explained by random variation: If $\mu$ represents the true rating for all movies and users and $\varepsilon$ represents independent errors sampled from the same distribution centered at zero, then:

$$
Y_{u,i} = \mu + \varepsilon_{u,i}
$$

In this case, the least squares estimate of $\mu$ — the estimate that minimizes the root mean squared error — is the average rating of all movies across all users.

```{r naive_rmse}
mu <- mean(train$rating)
```

In this first model, we will predict all ratings with the average rating of the training set and compare it to the actual rating values in the test set (`RMSE(test$rating, mu)`).

```{r naive_results, echo=FALSE}
rmse_results <- tibble(Method = "Just the Average", RMSE = RMSE(test$rating, mu))
kbl(rmse_results, booktabs = TRUE, caption = 'RMSE result for the Average Method') %>%  kable_styling(latex_options = c("striped","hold_position"))
```

we can observe that the value obtained was not that acceptable.

### Movie Effects Model

We can improve our model by adding a term, $b_i$, that represents the average rating for movie i:

$$
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
$$

Here, $b_i$ is the average of $Y_{u,i}$ minus the overall mean for each movie i. Note that because there are thousands of b's, the `lm()` function will be very slow or cause R to crash, so we don’t recommend using linear regression to calculate these effects.

```{r movie_effects}
movie_avgs <- train %>% group_by(movieId) %>% summarize(b_i = mean(rating-mu))

predicted_ratings <- mu + test %>% left_join(movie_avgs, by='movieId') %>%
  pull(b_i)

summary(predicted_ratings)
```

From the summary above, we can see that the values obtained for predicted ratings range from 0.5 and 4.75, inside the real range (0.5-5). Calculating the RMSE: 

```{r movie_effects_results, echo=FALSE}
RMSE(test$rating, predicted_ratings)

rmse_results <- bind_rows(rmse_results,
                          tibble(Method="Movie Effect Model",  
                                 RMSE = RMSE(test$rating, predicted_ratings)))
kbl(rmse_results, booktabs = TRUE, caption = 'RMSE results with the Movie Effect Model') %>%  kable_styling(latex_options = c("striped","hold_position"))
```

This is a big improvement, but we can do better. 

### Movie + User Effects Model

We can further improve our model by adding $b_u$, the user-specific effect:

$$ 
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
$$
$b_u$ is the average of $Y_{u,i}$ minus the overall rating for each movie i and the movie-specific coefficient. We will follow the same approach of the model before, as showed in the code chunk below: 

```{r m_u_effects}
user_avgs <- train %>%  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%  summarize(b_u = mean(rating - mu - b_i))

predicted_ratings <- test %>%
  left_join(movie_avgs, by='movieId') %>%  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%  pull(pred)

summary(predicted_ratings)
```

Predicted ratings now have values ranging from -0.7161 to  6.0547, which is bigger than the real range. There are `r sum(predicted_ratings>5)` values under 0.5 and `r sum(predicted_ratings>5)` over 5. But what we really need to evaluate is the RMSE value by applying `RMSE(test$rating, predicted_ratings)`.

```{r m_u_effects_results, echo=FALSE}
rmse_results <- bind_rows(rmse_results,
                          tibble(Method="Movie + User Effects Model",  
                                 RMSE = RMSE(test$rating, predicted_ratings)))
kbl(rmse_results, booktabs = TRUE, caption = 'RMSE results with the Movie and User Effects Model') %>%  kable_styling(latex_options = c("striped","hold_position"))

rm(predicted_ratings)
```

Another big improvement in the RMSE value. Let's proceed with other models.

### Movie + User + Genre Effects Model

Now we try an approach, proposed by Prof. Irrizarry in his book, that adds the genre effect to it. It was presented in the book but not utilized. As it was presented, the formula is written as shown below:

$$
Y_{u,i} = \mu + b_i + b_u + \sum_{k=1}^K x_{u,i} \beta_k + \varepsilon_{u,i}
$$

with $x^k_{u,i} = 1$ if $g_{u,i}$ is genre $k$. We already determined $x_{u,i}$ values earlier by creating binary columns for each of the 20 genres available in the training and test data. Now we determine $\beta_k$ values. We do it with the same approach used in the last two models, and each $\beta_k$ is the rating $Y_{u,i}$ minus the overall average rating, the movie-specific coefficient $b_i$ and user-specific coefficient $b_u$. To make all the process easier down the line, we will add $b_i$ and  $b_u$ to the training and test sets, and then we calculate all $\beta_k$'s.

```{r beta_genre}
train <- train %>% left_join(movie_avgs, by='movieId') %>% 
  left_join(user_avgs, by='userId') 
test <- test %>% left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId')
rm(user_avgs,movie_avgs)

beta_k <- vector() # empty vector
for (i in seq_along(all_genres)) {
  b_value<- train %>% 
    group_by(!!sym(all_genres[[i]])) %>%          
    summarize(beta_k=mean(rating-mu-b_i-b_u)) %>% 
    filter((.[1])==1) %>% .[[2]]                  
  beta_k <- append(beta_k, b_value)               
}
rm(i,b_value)

df_beta<-data.frame(beta_k) 
rownames(df_beta) <- all_genres
```

unquoting with !! and sym, beta value for all_genre[i], filter for all_genre[i]==1 and pulling beta value, appending it to beta vector. Transforming it into a dataframe.

```{r df_beta_divided, echo=FALSE}
df_beta_div <-data.frame(row.names(df_beta)[1:10], df_beta[1:10,],
                         row.names(df_beta)[11:20], df_beta[11:20,]) # Dividing table in two for better placement in report
colnames(df_beta_div) <- c('genre','beta_k','genre','beta_k')
df_beta_div %>% kbl(booktabs = TRUE, linesep = "", caption = 'Genre-specific beta values') %>% column_spec(1:4, width='8em') %>% kable_styling(latex_options = c("striped","hold_position")) %>% add_header_above(c("df_beta[1:10]" = 2, "df_beta[11:20]" = 2))
```

The sum inside the prediction equation with genre effect is represented below:

$$
\sum_{k=1}^K x_{u,i}\beta_{k} = x^{\{1\}}_{u,i}\beta_{1} +  x^{\{2\}}_{u,i}\beta_{2} + ... +  x^{\{20\}}_{u,i}\beta_{20}
$$

We apply the sum above by performing a matrix multiplication between the test set genre columns (`test[,4:23]`) and all 20 $\beta_k$ values calculated. This multiplication between a `r dim(test)[1]` per 20 matrix and a 20 per 1 vector results in a `r dim(test)[1]` per 1 vector. We then add this as a column to the test set, so we can calculate the predictions.

```{r m_u_g_effects, warning=FALSE}
sum_x_beta <- as.matrix(test[,4:23])%*%as.matrix(df_beta) 
sum_x_beta <- data.frame(sum_x_beta=sum_x_beta[,1])       

test <- data.frame(test, sum_x_beta)
rm(df_beta,beta_k,sum_x_beta,all_genres)

predicted_ratings <- test %>%
  mutate(pred = mu + b_i + b_u + sum_x_beta) %>%
  pull(pred)

summary(predicted_ratings)
```

Analyzing the values obtained, we see there are `r sum(predicted_ratings<0.5)` values under 0.5 and `r sum(predicted_ratings>5)` predicted ratings over 5.

```{r m_u_g_effects_results, echo=FALSE}
rmse_results <- bind_rows(rmse_results,
                          tibble(Method="Movie + User + Genre Effects Model",  
                                 RMSE = RMSE(test$rating, predicted_ratings)))
kbl(rmse_results, booktabs = TRUE, caption = 'RMSE results with the Movie, User and Genre Effects Model') %>%  kable_styling(latex_options = c("striped","hold_position"))
rm(predicted_ratings)
```

There is very little improvement from the last model, while the work needed to obtain it was really arduous.

### Regularized Movie + User Effects Model

We will now add regularization to one of the last models. Since the genre addition didn't have much effect on the RMSE and both $\beta_k$ calculations and optimal lambda determination are costly, in a computational point of view, we use the Movie + User Effects model.

penalized least squares

$$
\sum_{u,i} \left(y_{u,i} - \mu - b_i - b_u \right)^2 + 
\lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2\right)
$$

$$
\hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)
$$
$$
\hat{b}_u(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{b}_{i} - \hat{\mu}\right)
$$

```{r regularized_m_u_effects, fig.height=3, fig.fullwidth=TRUE , fig.align='center', fig.cap='RMSE values per lambda'}
train <- train %>% select(userId, movieId, rating)
test <- test %>% select(userId, movieId, rating)

lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  b_i <- train %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))       
  b_u <- train %>%
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l)) 
  predicted_ratings <-
    test %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)                                      
  return(RMSE(test$rating, predicted_ratings))
})
qplot(lambdas, rmses, xlab='lambda', ylab='RMSE')
```

optimal lambda is `r lambdas[which.min(rmses)]` and its corresponding RMSE is `r min(rmses)`.

```{r optimal_trained_model, echo=FALSE}
b_i <- train %>% group_by(movieId) %>%
summarize(b_i = sum(rating - mu)/(n()+5))

b_u <- train %>% left_join(b_i, by="movieId") %>% group_by(userId) %>%
summarize(b_u = sum(rating - b_i - mu)/(n()+5)) 

predicted_ratings <- test %>%
left_join(b_i, by = "movieId") %>%
left_join(b_u, by = "userId") %>%
mutate(pred = mu + b_i + b_u) %>%
pull(pred)                                      

summary(predicted_ratings)
```

`r sum(predicted_ratings<0.5)` and `r sum(predicted_ratings>5)`.

```{r lambda_rmse_results, echo=FALSE}
rmse_results <- bind_rows(
  rmse_results,
  tibble(Method="Regularized Movie + User Effects Model",  
         RMSE = min(rmses)))
kbl(rmse_results, booktabs = TRUE, caption = 'RMSE results with the Regularized Movie and User Effects Model') %>%
  kable_styling(latex_options = c("striped","hold_position"))

rm(lambdas,rmses,mu) # Cleaning variables to recover RAM space
```

### Matrix Factorization with _recosystem_ package

Now with a different approach, we try applying matrix factorization. P is the user index and Q the movie index. the _recosystem_ easily performs the factorization.

```{r recosystem_training, warning=FALSE, message=FALSE, results='hide'}
train_dm <- data_memory(user_index = train$userId, item_index = train$movieId,
                          rating = train$rating, index1 = TRUE) 
rm(train)

test_dm <- data_memory(user_index = test$userId, item_index = test$movieId, 
                       index1 = TRUE)                           
test_rating <- test$rating                                      
rm(test)

set.seed(123, sample.kind = "Rounding")
r <- Reco()                            
params = r$tune(train_dm, opts = list(dim = c(15, 20),
                                        costp_l1 = 0, #c(0, 0.1),
                                        costp_l2 = c(0.01, 0.1),
                                        costq_l1 = 0, #c(0, 0.1),
                                        costq_l2 = c(0.01, 0.1),
                                        lrate = c(0.075, 0.1), nthread = 2))

optimal_params = params$min

r$train(train_dm, opts = c(optimal_params, nthread = 1, niter = 20))
```

`r optimal_params`

with the trained model, it is possible to predict the ratings for the test set.

```{r predicting_trainset, message=FALSE}
predicted_ratings = r$predict(test_dm, out_memory()) 

summary(predicted_ratings)
```

`r sum(predicted_ratings<0.5)` and `r sum(predicted_ratings>5)`.

```{r MF_results, echo=FALSE}
RMSE(test_rating, predicted_ratings)

rmse_results <- bind_rows(rmse_results,
                          tibble(Method="Matrix Factorization Model",  
                                 RMSE = RMSE(test_rating, predicted_ratings)))

kbl(rmse_results, booktabs = TRUE, linesep = "", caption = 'RMSE results with the Matrix Factorization Model') %>%  
  kable_styling(latex_options = c("striped","hold_position"))

rm(test_rating,predicted_ratings,params,test_dm)
```

# Results

## Training with full edX dataset

To guarantee the prediction model can be applied to the whole validation set, we will train with full edX set.

```{r recosystem_edx, message=FALSE, results='hide'}
train_dm <- data_memory(user_index = edx$userId, item_index = edx$movieId, 
                          rating = edx$rating, index1 = TRUE)
rm(edx)

r$train(train_dm, opts = c(optimal_params, nthread = 1, niter = 20))
```

predicting for validation dataset, we get the following values:

```{r prediction_edx, message=FALSE}
validation_dm <- data_memory(user_index = validation$userId,
                             item_index = validation$movieId, index1 = TRUE)
validation_rating <- validation$rating

predicted_ratings = r$predict(validation_dm, out_memory()) 
rm(train_dm,validation)

summary(predicted_ratings)
```

`r sum(predicted_ratings<0.5)` and `r sum(predicted_ratings>5)`.

```{r results_validation, echo=FALSE}
RMSE(validation_rating, predicted_ratings)

rmse_validation <- tibble(Method="Matrix Factorization Model",  
                                 RMSE = RMSE(validation_rating, predicted_ratings))
colnames(rmse_validation) <- c('Method','Validation RMSE')
kbl(rmse_validation, booktabs = TRUE, caption = 'RMSE result for the validation set with Matrix Factorization Model') %>%  kable_styling(latex_options = c("striped","hold_position"))
```


# Conclusion

We visualized  the importance of variability between different movies and users, the genre  and genre combinations effect on ratings, some not that significant. A lot of models were evaluated, the genre addition didn't have a great effect on the model so we didn't utilize it during the regularized trial, and the best and easiest model to apply was Matrix Factorization with _recosystem_.


